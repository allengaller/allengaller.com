{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","path":"css/images/banner-bak.jpg","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1484147075000},{"_id":"themes/landscape/.DS_Store","hash":"9457f542cf2c3f2a50b9ecd64858fc6f50b0d0c4","modified":1484147080000},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1484126821000},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1484126821000},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1484126821000},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1484126821000},{"_id":"themes/landscape/_config.yml","hash":"285fb948f9f50e0d2c040283251b1ce20119bfaa","modified":1484214819000},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1484126821000},{"_id":"source/about/index.md","hash":"bfee1378ee8b0c03381c10c44157efb661c62430","modified":1484214601000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1484211471000},{"_id":"source/_posts/bigdata--portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/bigdata-hadoop-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/cloud--portal.md","hash":"ea990afde799dc9c6bf60015bf9668abd9c4d7d0","modified":1486108444000},{"_id":"source/_posts/bigdata-ml-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/bigdata-storm-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/cloud-paas-cf-core.md","hash":"a77e50fa1b3f67ada1bc5323779e6b3cd98b6cb7","modified":1484152524000},{"_id":"source/_posts/cloud-iaas-portal.md","hash":"5e405ec4afebf9d4a0beda52d73247535c65f739","modified":1484152398000},{"_id":"source/_posts/cloud-paas-openshift-core.md","hash":"db53a3ce03df639c195d89c45d70b0f0466d1020","modified":1484152520000},{"_id":"source/_posts/cloud-iaas-openstack-core.md","hash":"a4dfe305ed994558c7b4fc58de795922c7799462","modified":1484152388000},{"_id":"source/_posts/cloud-paas-portal.md","hash":"1a96b1ddfa2eb02998c00983879ee03adbe053f2","modified":1484152535000},{"_id":"source/_posts/cmd_django_docker.txt","hash":"278e19844de45ed90b64d49059539be018bb3fd4","modified":1484211486000},{"_id":"source/_posts/db--portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/db-mysql-portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/db-nosql-portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/cloud-saas-portal.md","hash":"1a96b1ddfa2eb02998c00983879ee03adbe053f2","modified":1484152535000},{"_id":"source/_posts/docker-cloud-detail.md","hash":"08f6e57862e8033af8373ef1ac51da8ffdd9faa6","modified":1484152346000},{"_id":"source/_posts/docker--portal.md","hash":"1636ea5ee9554eac1f5788794bab4e8899cd187d","modified":1484151372000},{"_id":"source/_posts/docker-compose-detail.md","hash":"3cd42fbe54ac2559af4af31dbacd44869aff9bfc","modified":1484723964000},{"_id":"source/_posts/docker-compose-file-detail.md","hash":"49be9993b1a9bbd740383c33e968a9daad204f70","modified":1484724794000},{"_id":"source/_posts/docker-compose-file-tmp.md","hash":"2bc28e5d777a9ce91daa76e3b563195d2a48da54","modified":1484724228000},{"_id":"source/_posts/docker-core.md","hash":"371801da82979044f60999285279fc46cc8c8b12","modified":1484461693000},{"_id":"source/_posts/docker-cookbook.md","hash":"8caa9454e90d4ade8268c325399b17a29c48c945","modified":1484304719000},{"_id":"source/_posts/devops--portal.md","hash":"e51f3a902a32e2295d38850467b740fe76786f1e","modified":1484144776000},{"_id":"source/_posts/docker-dockerfile-detail.md","hash":"a0e7e5e21c02381a9cd232349eed7eca7a492eaf","modified":1484308001000},{"_id":"source/_posts/docker-dockerhub-detail.md","hash":"d73c876795bd2175c53d186b816dd2585f00a254","modified":1484307994000},{"_id":"source/_posts/docker-engine-detail.md","hash":"b27a664d2c97671de05b37ee1667bde0487bd05f","modified":1484310053000},{"_id":"source/_posts/docker-filesystem-core.md","hash":"f57c094ccfa1ff199415a9bdc571f65fc25651e1","modified":1484465420000},{"_id":"source/_posts/docker-image-detail.md","hash":"21421ab56a9a54fe66765d998db6df3dab394563","modified":1484308232000},{"_id":"source/_posts/docker-k8s-core.md","hash":"c932617dd05b171229eb0cbad913f31974775902","modified":1484152331000},{"_id":"source/_posts/docker-machine-detail.md","hash":"1761bafb16454ec9b0b9035b9186ca436cc64cf4","modified":1484718613000},{"_id":"source/_posts/docker-mesos-core.md","hash":"08f6e57862e8033af8373ef1ac51da8ffdd9faa6","modified":1484152346000},{"_id":"source/_posts/docker-security-core.md","hash":"e132f49ec05126a1762374f7a7ad5909610680cb","modified":1484457984000},{"_id":"source/_posts/docker-network-core.md","hash":"4adbd26909d01e284da4302cfbe98dc794398caf","modified":1484457933000},{"_id":"source/_posts/docker-storage-core.md","hash":"aba71f3dda6a43005db308862b385778f0b7ddc3","modified":1484461617000},{"_id":"source/_posts/docker-swarm-portal.md","hash":"ce36aba7fb4f271d68ada5f3628f82fbe6fef12d","modified":1484466953000},{"_id":"source/_posts/docker-store-detail.md","hash":"08f6e57862e8033af8373ef1ac51da8ffdd9faa6","modified":1484152346000},{"_id":"source/_posts/docker-swarm-detail.md","hash":"d1ce7533dcdadbed6620c865f7193a22100c792f","modified":1484465445000},{"_id":"source/_posts/docker-swarmkit-detail.md","hash":"ecbe772c349efe2ac43cac2e81357eeabc7d99ed","modified":1484467444000},{"_id":"source/_posts/docker-yaml-detail.md","hash":"5ce41c15cd0ca11e8a6a621bc0c1a60e16c119fe","modified":1484458296000},{"_id":"source/_posts/hello-world.md","hash":"2821cef1e0b6723d18e7f2162653fe696c4dbdc0","modified":1484152366000},{"_id":"source/_posts/docker-swarmnext-detail.md","hash":"859394563adc3ede668fa8e47d0c826e4b249199","modified":1484486949000},{"_id":"source/_posts/iot-portal.md","hash":"a4dfe305ed994558c7b4fc58de795922c7799462","modified":1484152388000},{"_id":"source/_posts/jd-architect-portal.md","hash":"6932aaea71078034615157134e80029d70c28bcd","modified":1484308101000},{"_id":"source/_posts/jd-fullstack-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/jd--portal.md","hash":"6932aaea71078034615157134e80029d70c28bcd","modified":1484308101000},{"_id":"source/_posts/lang--portal.md","hash":"2cd25c7e83b0f07406bfa8dfc5ea285c1e1936ce","modified":1484152314000},{"_id":"source/_posts/lang-cx-portal.md","hash":"2cd25c7e83b0f07406bfa8dfc5ea285c1e1936ce","modified":1484152314000},{"_id":"source/_posts/lang-java-portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/lang-js-nodejs-core.md","hash":"c3d8104fd5c3ba26bd0215b83a6f169248a6921c","modified":1484152422000},{"_id":"source/_posts/lang-js-portal.md","hash":"c99c24b65ab554ef3d4bb3a145ed011f17f5e369","modified":1484152428000},{"_id":"source/_posts/lang-python-django-core.md","hash":"b58bfdbcca877f4c3041e6e25fa36b7f2be82140","modified":1484214459000},{"_id":"source/_posts/lang-python-django-install.md","hash":"7f922a7b5eb9ee80cb5cd6884eb3d87f81a0d595","modified":1484214542000},{"_id":"source/_posts/lang-python-portal.md","hash":"0f1a90cb1de64442035b364589c1ffa2fd49ef4e","modified":1484214141000},{"_id":"source/_posts/lang-ruby-rails-core.md","hash":"37bf54a843a2fb191bdc49ca6c2c504750db7df7","modified":1484152581000},{"_id":"source/_posts/microservice-portal.md","hash":"5ed1f32afcdff48260522df9c824d06dd89fe368","modified":1484458179000},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1484126821000},{"_id":"source/_posts/network-http-core.md","hash":"a5853d8ae8a690d707ee6e5e669ea3470c492940","modified":1484152471000},{"_id":"source/_posts/network-portal.md","hash":"1d97325d88b500484030a30cafd44c41b51c358c","modified":1484486959000},{"_id":"source/_posts/system-linux-portal.md","hash":"4e023a556e18a248bb932ff4067513be2779b1c0","modified":1484152446000},{"_id":"themes/landscape/source/.DS_Store","hash":"1429b9c08aeb8c15b3499d150e011720f8ff275c","modified":1484147087000},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1484126821000},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1484126821000},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1484126821000},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1484126821000},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1484126821000},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1484126821000},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1484126821000},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1484126821000},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1484126821000},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1484126821000},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1484126821000},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1484126821000},{"_id":"themes/landscape/source/css/_variables.styl","hash":"06e2f44b92c26c5d71abf01e7b43ee0dfd2010c7","modified":1484148828000},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1484126821000},{"_id":"themes/landscape/source/css/.DS_Store","hash":"0c4a669591bf1723e84d44bb15e7b684e25ff531","modified":1484146229000},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"9292c640bdf7c8eb6fed2e8a1800f1cc7f43722b","modified":1484145585000},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"3ff1260ab513c523a610f1d83b20961b5d140d6b","modified":1484213973000},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"f6975a227829834c026b17ee2493d06a16202b94","modified":1484144946000},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"bfd64f2a831a6acb7f5bae852cae3098a91e1997","modified":1484145371000},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"9d7b18ae2a5479d9ae0eb053ea7043ab8a9bd642","modified":1484145235000},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1484126821000},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1484146333000},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1484126821000},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"9658cb416b434dc6c3a8c2c15511eb170f363a3d","modified":1484148524000},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1484213953000},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1484126821000},{"_id":"public/about/index.html","hash":"039b767acebece2e4a03fbaf922ecb1836459a83","modified":1486131988514},{"_id":"public/python/lang-python-django-install/index.html","hash":"336b7301a719f85bec5694c9401b3c9e8f806f17","modified":1486131988514},{"_id":"public/docker/docker-machine-detail/index.html","hash":"2b490e1171b215de4d90d3edeee19c44101b8418","modified":1486131988514},{"_id":"public/python/bigdata-ml-portal/index.html","hash":"cdae9965c33b72ad42436868e6ec87d7700b98f4","modified":1486131988514},{"_id":"public/python/bigdata-storm-portal/index.html","hash":"42def3d74f9f4c089c063710314e43695dc7148a","modified":1486131988514},{"_id":"public/paas/cloud-paas-cf-core/index.html","hash":"bbcf8abc106598e84e758ba01dfcc3d76273801b","modified":1486131988514},{"_id":"public/paas/cloud-paas-openshift-core/index.html","hash":"e4500823ec81b31d794d88e874ed8ff034b70473","modified":1486131988514},{"_id":"public/iaas/cloud-iaas-portal/index.html","hash":"4131e035532470b71e2ca1313aa196bbf7e1c14d","modified":1486131988515},{"_id":"public/iaas/cloud-iaas-openstack-core/index.html","hash":"dc345b77cf5045a5df8a2e02981b549c1924bae8","modified":1486131988515},{"_id":"public/paas/cloud-paas-portal/index.html","hash":"575ce95c7726d4d74de9d217b6bf2630ab13cd8b","modified":1486131988515},{"_id":"public/java/db--portal/index.html","hash":"ccf270dc1190ff42405403d777b6987f48f3f22e","modified":1486131988515},{"_id":"public/java/db-mysql-portal/index.html","hash":"bec0c6982f80c662685da149815c8a15e7a72d95","modified":1486131988515},{"_id":"public/java/db-nosql-portal/index.html","hash":"b2d3ecfe7e96ba1ff59c991d6ce69ab6dad369bb","modified":1486131988515},{"_id":"public/docker/docker-cloud-detail/index.html","hash":"93ac00df1a2c71e39d8ead7e1fe3e7a6e766d46e","modified":1486131988515},{"_id":"public/paas/cloud-saas-portal/index.html","hash":"a0130257cc01037b155b0776c2e40c2da6bd5e57","modified":1486131988515},{"_id":"public/docker/docker-compose-detail/index.html","hash":"052cd60726cbdac0657808cfadb8a4d77f5bb63d","modified":1486131988515},{"_id":"public/docker/docker-compose-file-detail/index.html","hash":"032f838e9ce43903dd0d5ac145f93ed11206aa4f","modified":1486131988515},{"_id":"public/python/bigdata--portal/index.html","hash":"408421e7a8b92955e9d45804c39a427a72816f71","modified":1486131988515},{"_id":"public/docker/docker-core/index.html","hash":"4094848bf94fedb4f1d64d185fa3b8680d59b561","modified":1486131988515},{"_id":"public/docker/docker-cookbook/index.html","hash":"aef832a765c6934f1b8c1146d7d55ccb21f876e3","modified":1486131988515},{"_id":"public/devops/devops--portal/index.html","hash":"79155385077b759e09209aae991682afe959b077","modified":1486131988515},{"_id":"public/docker/docker-dockerfile-detail/index.html","hash":"effd56245564255d7f6c00fe244460e2e2757516","modified":1486131988515},{"_id":"public/docker/docker-dockerhub-detail/index.html","hash":"a9559100e8fef720d87e1a8c9941c50143b32842","modified":1486131988515},{"_id":"public/docker/docker-engine-detail/index.html","hash":"5fed36fb192f8e45b0de17d1d80e4ba73976f8e0","modified":1486131988515},{"_id":"public/docker/docker-filesystem-core/index.html","hash":"9c7d707f84c4028f40e1aedd8959d26c1dddd559","modified":1486131988516},{"_id":"public/docker/docker-image-detail/index.html","hash":"416d6b98492c89669d930b4c608501303ad01d7f","modified":1486131988516},{"_id":"public/docker/docker-k8s-core/index.html","hash":"2199a14c910e0c5c2414554f5bf84547bdec320b","modified":1486131988516},{"_id":"public/python/bigdata-hadoop-portal/index.html","hash":"4470ccdd1f3201a97814f7ca3fdce9bb1e95d964","modified":1486131988516},{"_id":"public/docker/docker-mesos-core/index.html","hash":"fdceea7efe3e57ba91b2f8757aeb89e9e3cdfc68","modified":1486131988516},{"_id":"public/docker/docker-security-core/index.html","hash":"a90e96b29f1e77290c9523e4d83830864a97974d","modified":1486131988516},{"_id":"public/docker/docker-network-core/index.html","hash":"c3aaa0580b399cfb3ccfe9747e3a4e9f5662b1c1","modified":1486131988516},{"_id":"public/docker/docker-storage-core/index.html","hash":"eff4a62ce2deb7486b636f2edb94f36834009b4c","modified":1486131988516},{"_id":"public/docker/docker-swarm-portal/index.html","hash":"e4d9ee7c62bb1c3da8afc53599a66650f5338afa","modified":1486131988516},{"_id":"public/docker/docker-store-detail/index.html","hash":"bd47404074b3a183eae620c412b54f0cebc4c56b","modified":1486131988516},{"_id":"public/docker/docker-swarm-detail/index.html","hash":"e970a12557e7450a39cc280084a40c2b11ecc5e1","modified":1486131988516},{"_id":"public/iaas/docker-swarmkit-detail/index.html","hash":"06bd10e0840018652e27993ea281360a817c583d","modified":1486131988516},{"_id":"public/docker/docker-yaml-detail/index.html","hash":"c7a311a2ad99feeec188cda72b30e2fdc058385f","modified":1486131988516},{"_id":"public/network/network-portal/index.html","hash":"e62ec98985e7aba79b9bffc89ea0990805182f58","modified":1486131988516},{"_id":"public/docker/docker-swarmnext-detail/index.html","hash":"78301a8e601f4d8691c4c94385f23a8b2da7eb3a","modified":1486131988516},{"_id":"public/iaas/iot-portal/index.html","hash":"5023667f8c1aff882429b01694f4dc734760dc2a","modified":1486131988516},{"_id":"public/cloud/jd-architect-portal/index.html","hash":"f0e78c279769de47617306e0d25ba96611768377","modified":1486131988516},{"_id":"public/python/jd-fullstack-portal/index.html","hash":"6f2fb73efb038e778c1ec03dcc30cfa73175a538","modified":1486131988517},{"_id":"public/cloud/jd--portal/index.html","hash":"dddf22e274a4ca2551a9c905b37b428674904a0e","modified":1486131988517},{"_id":"public/cx/lang--portal/index.html","hash":"5c4773c98207352c4c08d227a28100f26dee625d","modified":1486131988517},{"_id":"public/cx/lang-cx-portal/index.html","hash":"3e23cf92681fc57377184331ebbd64e80c0103ee","modified":1486131988517},{"_id":"public/java/lang-java-portal/index.html","hash":"1287225d25b65415f0f810aa5971a653bc830069","modified":1486131988517},{"_id":"public/nodejs/lang-js-nodejs-core/index.html","hash":"2967e0716e860d65927c1a95d9fb36b10a0564da","modified":1486131988517},{"_id":"public/js/lang-js-portal/index.html","hash":"4e6b89631c51096f1d093792810f613fff1db028","modified":1486131988517},{"_id":"public/python/lang-python-django-core/index.html","hash":"9eefb46abcb19f0459fd1348eee8b3b405e28915","modified":1486131988517},{"_id":"public/python/cloud--portal/index.html","hash":"6783cc23816c99f2a379655dc7810624c2a12c65","modified":1486131988517},{"_id":"public/python/lang-python-portal/index.html","hash":"f851451e46d7653e9ba8a19d40698268f266ccd6","modified":1486131988517},{"_id":"public/microservice/microservice-portal/index.html","hash":"7494af9df8a0fc1b3c650f29bef7cc0ca94a18c8","modified":1486131988517},{"_id":"public/ruby/lang-ruby-rails-core/index.html","hash":"19244df81b9add9f81dc9120dd02d2db6d91142b","modified":1486131988517},{"_id":"public/network/network-http-core/index.html","hash":"33ba283c2a1407d9a26c1e7a785ceea1e25b1fa0","modified":1486131988517},{"_id":"public/linux/system-linux-portal/index.html","hash":"03af121cd58afc99f9ce3c3df0dca8b49ceb0330","modified":1486131988517},{"_id":"public/tmp/hello-world/index.html","hash":"9def59e437851a159fa2265bcdd26561fe494d00","modified":1486131988517},{"_id":"public/archives/index.html","hash":"cea3b8874d7313317c0e1c5becc14cd78bdfe475","modified":1486131988517},{"_id":"public/archives/page/2/index.html","hash":"fb641a965da590f258685e4fce416dd405536a29","modified":1486131988517},{"_id":"public/archives/page/3/index.html","hash":"684ae16adee0fbe24492617f4aa5787e1dab3b53","modified":1486131988517},{"_id":"public/archives/page/4/index.html","hash":"e132faa287d4aaaac3bc3c87ab97d57b4567781b","modified":1486131988517},{"_id":"public/archives/page/5/index.html","hash":"2b84b93e14ba9db3cd392ea69d06f2851eb0dec1","modified":1486131988517},{"_id":"public/archives/page/6/index.html","hash":"79ba92caff70e5e5851a58c0b887a66e3513ccc8","modified":1486131988517},{"_id":"public/archives/2017/index.html","hash":"4f87f112c65b577575a93650626c5b77def82e6c","modified":1486131988517},{"_id":"public/archives/2017/page/2/index.html","hash":"57ff6c534a634c4c335fd43b79246007144eab12","modified":1486131988517},{"_id":"public/archives/2017/page/3/index.html","hash":"3fa76c04a59454877e0e8e753e1bd5a9c94fcc10","modified":1486131988517},{"_id":"public/archives/2017/page/4/index.html","hash":"2f907b96d8d58d8e08c6ef886475deb5e26663d4","modified":1486131988518},{"_id":"public/archives/2017/page/5/index.html","hash":"d1637a562280cb54c30249b04414cd4ff8d7a7c0","modified":1486131988518},{"_id":"public/archives/2017/page/6/index.html","hash":"5dcdf7649ffd4126deabe46d9a9a32d584b30d5e","modified":1486131988519},{"_id":"public/archives/2017/01/index.html","hash":"66993604fa6ed0c14d4dd84bd18aa006c5aaf4c6","modified":1486131988519},{"_id":"public/archives/2017/01/page/2/index.html","hash":"afe49f654c468e33bcdfc1775a600eb6e2d74572","modified":1486131988519},{"_id":"public/archives/2017/01/page/3/index.html","hash":"a8546e0cff074d6591ecf0d70ab6cbe3ea3c7e8a","modified":1486131988519},{"_id":"public/archives/2017/01/page/4/index.html","hash":"04f48a9435c58c21d4a0f3158e6182e06149954d","modified":1486131988519},{"_id":"public/archives/2017/01/page/5/index.html","hash":"e06502bf63a749e1ef62a8bfe4b77396624a0589","modified":1486131988519},{"_id":"public/archives/2017/01/page/6/index.html","hash":"e83abe343129b0333a11c205f7dd09406ef76d29","modified":1486131988519},{"_id":"public/categories/python/index.html","hash":"b4cd3c8590e6835a3a234aeabddfbe193a8f196d","modified":1486131988519},{"_id":"public/categories/paas/index.html","hash":"bf77cb4b4fb4848860f54cc041b211a5367bb2a8","modified":1486131988519},{"_id":"public/categories/iaas/index.html","hash":"3bc9747344c04e0189c991ee501c3c9fd3f9b621","modified":1486131988519},{"_id":"public/categories/java/index.html","hash":"b3e4d232a92d899b4ba53620f21cd4286f693701","modified":1486131988519},{"_id":"public/categories/docker/index.html","hash":"59a12e71cae3679306f1f5bca1683c2f192068fc","modified":1486131988519},{"_id":"public/categories/docker/page/2/index.html","hash":"528ac54a207c617d2a2f4ad588e38bbab33ee9dd","modified":1486131988519},{"_id":"public/categories/docker/page/3/index.html","hash":"3044d31055ba232ac8e16e2bd9cb03b13f3035ef","modified":1486131988519},{"_id":"public/categories/devops/index.html","hash":"bd4968064ae18ab07269de806d163066438952ea","modified":1486131988519},{"_id":"public/categories/tmp/index.html","hash":"4943f8a19f1118cb96e3292a8e489cfb43998a7c","modified":1486131988519},{"_id":"public/categories/cloud/index.html","hash":"24e6c7607df8c7c50b94435463b9542950eab7f3","modified":1486131988519},{"_id":"public/categories/cx/index.html","hash":"cd7c1dd0a6ad20a03ce05e1ce82836470a2a041c","modified":1486131988519},{"_id":"public/categories/nodejs/index.html","hash":"e63be2c0fd0049c64afdfce800f54b9a673b601c","modified":1486131988519},{"_id":"public/categories/js/index.html","hash":"335ecb5eb4421fc0a1c23bdfab6328c7c1c78082","modified":1486131988519},{"_id":"public/categories/microservice/index.html","hash":"688abbb94199d15f0e84621d333d4556837cc33a","modified":1486131988519},{"_id":"public/categories/ruby/index.html","hash":"641df9b1706f95b365c47b37804261b3f000b341","modified":1486131988519},{"_id":"public/categories/network/index.html","hash":"127a7f808340cb46500c10830e201e6580750f68","modified":1486131988519},{"_id":"public/categories/linux/index.html","hash":"6a26ee3881ed9dc89063fecccf23b759493c6db4","modified":1486131988520},{"_id":"public/tags/portal/index.html","hash":"cabaa3a8f0db809c6342c94a6132ad783bd22309","modified":1486131988520},{"_id":"public/tags/portal/page/2/index.html","hash":"d62d7e850469597039925643e73cc91057f51dce","modified":1486131988520},{"_id":"public/tags/portal/page/3/index.html","hash":"72c696c807f14da23d53d3e52e5d4b318f4f63f0","modified":1486131988520},{"_id":"public/tags/core/index.html","hash":"ee6fab4e06c4a4d5380e6ff49fdfc66c31b77b3f","modified":1486131988520},{"_id":"public/tags/core/page/2/index.html","hash":"96f83807e926a3722d8abce30c857a3e0fee4030","modified":1486131988520},{"_id":"public/tags/cloudfoundry/index.html","hash":"c3df26dd46e71159680086c08965bdb65851a9b5","modified":1486131988520},{"_id":"public/tags/openshift/index.html","hash":"ef31a8253c2849934c98708e7d150f3942f70684","modified":1486131988520},{"_id":"public/tags/openstack/index.html","hash":"5aeefd9849308785ed2dc8a7fa3b86cf3d8a337f","modified":1486131988520},{"_id":"public/tags/detail/index.html","hash":"bebeb20da8c18599196cf3c42a1bd7a02b9aafec","modified":1486131988520},{"_id":"public/tags/cookbook/index.html","hash":"2e60cbabb7d124c4bf4616fe1270014f2f1ef8d0","modified":1486131988520},{"_id":"public/tags/filesystem/index.html","hash":"6175f8d96ed6d9a6515afc18e852367b22549d98","modified":1486131988521},{"_id":"public/tags/security/index.html","hash":"a05ec2f21a2ef67bf23a335bd253e79b3e3eceab","modified":1486131988521},{"_id":"public/tags/network/index.html","hash":"eb9423dbe6f94e13bbae2ecd0f41be22057fb5ba","modified":1486131988521},{"_id":"public/tags/storage/index.html","hash":"b20e96470590cbff9de9f95692b7cf650c153318","modified":1486131988521},{"_id":"public/tags/swarm/index.html","hash":"7979bcd11d0a8749dcc0a2576575ccddacc3e221","modified":1486131988521},{"_id":"public/tags/architect/index.html","hash":"596f60d4ef97f94d67d9c858dcedd7751f94746c","modified":1486131988521},{"_id":"public/tags/django/index.html","hash":"06e5e9ac5cb610944d950699488f08fef757bf07","modified":1486131988521},{"_id":"public/tags/install/index.html","hash":"3275ce22c058255356f318f243f1efe521e076eb","modified":1486131988522},{"_id":"public/tags/python/index.html","hash":"0dd3dd06c9ef73657a000651137451f465ce6214","modified":1486131988522},{"_id":"public/tags/docker/index.html","hash":"46a52d2d5a1a1525b6fb72636dfb5043fa7ff379","modified":1486131988522},{"_id":"public/tags/rails/index.html","hash":"7cdee3370f76b4c52477cb2edabb479f39729f3c","modified":1486131988522},{"_id":"public/tags/http/index.html","hash":"b46ad367b16fd221fd32b6448fe0e3faeb9bc124","modified":1486131988522},{"_id":"public/uncategorized/docker-compose-file-tmp/index.html","hash":"0f3c3ee8248962b513d82007c2ced1f97e3e9279","modified":1486131988522},{"_id":"public/docker/docker--portal/index.html","hash":"1663e515f61f10d5491d6e774f37d362a5d13fa3","modified":1486131988522},{"_id":"public/index.html","hash":"e9ac184685e0e52a40ea65e347324a6b85b5193a","modified":1486131988522},{"_id":"public/page/2/index.html","hash":"8e68d3d89bc73f85583d3c4788453e19c4911473","modified":1486131988522},{"_id":"public/page/3/index.html","hash":"6097a54100346ac097d9fd3c8dcb2fd2f496ebde","modified":1486131988522},{"_id":"public/page/4/index.html","hash":"676d6d9bc902f001ebcbc14080ff604f69523902","modified":1486131988522},{"_id":"public/page/5/index.html","hash":"6aaf7b7c925fdae5dbdbe667017137e020ece708","modified":1486131988522},{"_id":"public/page/6/index.html","hash":"b3187206a40aaf95f7b4976261d86f3f2f42d812","modified":1486131988522},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1486131988537},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1486131988537},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1486131988537},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1486131988537},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1486131988537},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1486131988537},{"_id":"public/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1486131988537},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1486131988537},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1486131988537},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1486131988537},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1486131988537},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1486131990461},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1486131990470},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1486131990470},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1486131990470},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1486131990470},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1486131990470},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1486131990470},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1486131990470},{"_id":"public/css/style.css","hash":"f1bdee26c06f8dca10fa0df089e978e0469d755a","modified":1486131990470},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1486131990470},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1486131990471},{"_id":"public/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1486131990480},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1486131990480}],"Category":[{"name":"python","_id":"ciypwm36s000321sv2ayq74mg"},{"name":"paas","_id":"ciypwm37q000n21sv6dlm59y0"},{"name":"iaas","_id":"ciypwm37x000v21svrncf6pj1"},{"name":"java","_id":"ciypwm38k001h21svxlgxac0o"},{"name":"docker","_id":"ciypwm396002421svh3qvjzup"},{"name":"devops","_id":"ciypwm3am003e21svty5ib55t"},{"name":"tmp","_id":"ciypwm3b3004121svtkx85z4d"},{"name":"cloud","_id":"ciypwm3b9004921svnu7nqm17"},{"name":"cx","_id":"ciypwm3bj004q21sve4wap485"},{"name":"nodejs","_id":"ciypwm3c0005621svwg9s5r8n"},{"name":"js","_id":"ciypwm3c6005e21sv23z2ug6y"},{"name":"microservice","_id":"ciypwm3cd005m21sva7pdlkq9"},{"name":"ruby","_id":"ciypwm3ce005u21sv403nox6t"},{"name":"network","_id":"ciypwm3cg006021svbgr4bjye"},{"name":"linux","_id":"ciypwm3ci006c21svajl2sgwk"}],"Data":[],"Page":[{"title":"about","date":"2017-01-12T17:49:37.000Z","_content":"\n123","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-01-12 17:49:37\n---\n\n123","updated":"2017-01-12T09:50:01.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"ciypwm35q000021sv7ev9hb2x","content":"<p>123</p>\n","excerpt":"","more":"<p>123</p>\n"}],"Post":[{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata--portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm36l000121sv4vi8ieps","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-hadoop-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-hadoop-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm36p000221svq1phbbk0","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"cloud portal","_content":"\n# top resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html","source":"_posts/cloud--portal.md","raw":"---\ntitle: cloud portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# top resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html","slug":"cloud--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-03T07:54:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm36y000521svf4gipt4m","content":"<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\" target=\"_blank\" rel=\"external\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\" target=\"_blank\" rel=\"external\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\" target=\"_blank\" rel=\"external\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\" target=\"_blank\" rel=\"external\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n","excerpt":"","more":"<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-ml-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-ml-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm370000621sv5wqww7o5","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-storm-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-storm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm373000721sve8sra0p8","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"cloudfoundry core","_content":"\n# ","source":"_posts/cloud-paas-cf-core.md","raw":"---\ntitle: cloudfoundry core\ncategories:\n- paas\ntags:\n- core\n- cloudfoundry\n---\n\n# ","slug":"cloud-paas-cf-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37b000b21svs9suqb35","content":"<p># </p>\n","excerpt":"","more":"<p># </p>\n"},{"title":"iaas core","_content":"\n# iaas core","source":"_posts/cloud-iaas-portal.md","raw":"---\ntitle: iaas core\ncategories:\n- iaas\ntags:\n- portal\n---\n\n# iaas core","slug":"cloud-iaas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37g000e21sv4w08jqwz","content":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1>","excerpt":"","more":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1>"},{"title":"openshift core","_content":"\n# ","source":"_posts/cloud-paas-openshift-core.md","raw":"---\ntitle: openshift core\ncategories:\n- paas\ntags:\n- core\n- openshift\n---\n\n# ","slug":"cloud-paas-openshift-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37k000j21svx26uzv21","content":"<p># </p>\n","excerpt":"","more":"<p># </p>\n"},{"title":"openstack core","_content":"\n# core","source":"_posts/cloud-iaas-openstack-core.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# core","slug":"cloud-iaas-openstack-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37n000m21svlowv6kfh","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"paas portal","_content":"\n","source":"_posts/cloud-paas-portal.md","raw":"---\ntitle: paas portal\ncategories:\n- paas\ntags:\n- portal\n---\n\n","slug":"cloud-paas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37s000r21svd81wuly4","content":"","excerpt":"","more":""},{"title":"java portal","_content":"\n#  portal","source":"_posts/db--portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"db--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37v000u21svtfny6hag","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"java portal","_content":"\n#  portal","source":"_posts/db-mysql-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"db-mysql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37z000y21sv7ku7mwqz","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"java portal","_content":"\n#  portal","source":"_posts/db-nosql-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"db-nosql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm381001121svtap4yfqx","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"mesos core","_content":"\n#  core","source":"_posts/docker-cloud-detail.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-cloud-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm387001621sv0tyyvktn","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"paas portal","_content":"\n","source":"_posts/cloud-saas-portal.md","raw":"---\ntitle: paas portal\ncategories:\n- paas\ntags:\n- portal\n---\n\n","slug":"cloud-saas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38b001821svqerj6t12","content":"","excerpt":"","more":""},{"title":"docker portal","_content":"\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\nmooc\n    https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","source":"_posts/docker--portal.md","raw":"---\ntitle: docker portal\ncategories:\n- docker\ntags:\n- portal\n---\n\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\nmooc\n    https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","slug":"docker--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:16:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38f001d21svebn6o2x0","content":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\" target=\"_blank\" rel=\"external\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\" target=\"_blank\" rel=\"external\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\" target=\"_blank\" rel=\"external\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\" target=\"_blank\" rel=\"external\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\" target=\"_blank\" rel=\"external\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\" target=\"_blank\" rel=\"external\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\" target=\"_blank\" rel=\"external\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\" target=\"_blank\" rel=\"external\">dockerpool</a><br><a href=\"http://dockerone.com\" target=\"_blank\" rel=\"external\">dockerone</a><br><a href=\"http://devops-china.org\" target=\"_blank\" rel=\"external\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\" target=\"_blank\" rel=\"external\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\" target=\"_blank\" rel=\"external\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\" target=\"_blank\" rel=\"external\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\" target=\"_blank\" rel=\"external\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\" target=\"_blank\" rel=\"external\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\" target=\"_blank\" rel=\"external\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\" target=\"_blank\" rel=\"external\"></a>, <a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"external\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\" target=\"_blank\" rel=\"external\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\" target=\"_blank\" rel=\"external\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\" target=\"_blank\" rel=\"external\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\" target=\"_blank\" rel=\"external\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\" target=\"_blank\" rel=\"external\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\" target=\"_blank\" rel=\"external\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\" target=\"_blank\" rel=\"external\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\" target=\"_blank\" rel=\"external\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>mooc<br>    <a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\" target=\"_blank\" rel=\"external\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\" target=\"_blank\" rel=\"external\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\" target=\"_blank\" rel=\"external\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\" target=\"_blank\" rel=\"external\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\" target=\"_blank\" rel=\"external\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\" target=\"_blank\" rel=\"external\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\" target=\"_blank\" rel=\"external\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\" target=\"_blank\" rel=\"external\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"http://help.daocloud.io/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\" target=\"_blank\" rel=\"external\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\" target=\"_blank\" rel=\"external\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\" target=\"_blank\" rel=\"external\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\" target=\"_blank\" rel=\"external\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\" target=\"_blank\" rel=\"external\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\" target=\"_blank\" rel=\"external\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\" target=\"_blank\" rel=\"external\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\" target=\"_blank\" rel=\"external\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\" target=\"_blank\" rel=\"external\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\" target=\"_blank\" rel=\"external\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\" target=\"_blank\" rel=\"external\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\" target=\"_blank\" rel=\"external\">https://chixq.com/articles/12-factor-app/</a></p>\n","excerpt":"","more":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\">dockerpool</a><br><a href=\"http://dockerone.com\">dockerone</a><br><a href=\"http://devops-china.org\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\"></a>, <a href=\"https://github.com/opencontainers\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>mooc<br>    <a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\"></a><br>  <a href=\"http://help.daocloud.io/\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\">https://chixq.com/articles/12-factor-app/</a></p>\n"},{"title":"docker compose","_content":"\n# about\n\n- official: https://docs.docker.com/compose/\n- in production: https://docs.docker.com/compose/production/\n\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start\n\n    Start services\n\n- docker-compose stop\n\n    Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","source":"_posts/docker-compose-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n- official: https://docs.docker.com/compose/\n- in production: https://docs.docker.com/compose/production/\n\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start\n\n    Start services\n\n- docker-compose stop\n\n    Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","slug":"docker-compose-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-18T07:19:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38h001f21sv8w2usj1k","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/</a></li>\n<li>in production: <a href=\"https://docs.docker.com/compose/production/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/production/</a></li>\n</ul>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\" target=\"_blank\" rel=\"external\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\" target=\"_blank\" rel=\"external\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start</p>\n<p>  Start services</p>\n</li>\n<li><p>docker-compose stop</p>\n<p>  Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/\">https://docs.docker.com/compose/</a></li>\n<li>in production: <a href=\"https://docs.docker.com/compose/production/\">https://docs.docker.com/compose/production/</a></li>\n</ul>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start</p>\n<p>  Start services</p>\n</li>\n<li><p>docker-compose stop</p>\n<p>  Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n"},{"title":"docker compose","_content":"\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution","source":"_posts/docker-compose-file-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution","slug":"docker-compose-file-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-18T07:33:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38l001k21svmq9vzpvt","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>"},{"_content":"    ```\n        containers:\n        web:\n         build: .\n         command: python app.py\n         ports:\n         - \"5000:5000\"\n         volumes:\n         - .:/code\n         links:\n         - redis\n         environment:\n         - PYTHONUNBUFFERED=1\n        redis:\n         image: redis:latest\n         command: redis-server --appendonly yes\n     ```\n\n     ```\n        wiki2:\n        image: 'nickstenning/mediawiki'\n        ports:\n            - \"8880:80\"\n        links:\n            - db:database\n        volumes:\n            - /data/wiki2:/data\n\n        db:\n        image: \"mysql\"\n        expose:\n            - \"3306\"\n        environment:\n            - MYSQL_ROOT_PASSWORD=defaultpass\n\n    上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n    第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n    定义完成后，通过下面的命令来启动应用：\n\n    docker-compose up\n\n    links指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n\n# Variable substitution\n            \nBoth $VARIABLE and ${VARIABLE} syntax are supported. \nExtended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.\n            db:\n  image: \"postgres:${POSTGRES_VERSION}\"\n            web:\n  build: .\n  command: \"$$VAR_NOT_INTERPOLATED_BY_COMPOSE\"\n        \n# install\n\n    curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk 'NR==1{print $NF}')/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n        doc\n            docker-compose.yml reference\n                https://docs.docker.com/compose/yml/\n        keywords\n            image\n                指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n                image: ubuntu\n            build\n                指定 Dockerfile 所在文件夹的路径\n                Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n                build: /path/to/build/dir\n            dockerfile\n            command\n                覆盖容器启动后默认执行的命令。\n                command: bundle exec thin -p 3000\n            links\n                链接到其它服务中的容器\n                使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                    links:\n - db\n - db:database\n - redis\n                使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n                    172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            external_links\n                链接到 docker-compose.yml 外部的容器\n                甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n            ports\n                暴露端口信息\n                使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n                ports:\n - \"3000\"\n - \"8000:8000\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n            expose\n                暴露端口，但不映射到宿主机，只被连接的服务访问\n                仅可以指定内部端口为参数\n                expose:\n - \"3000\"\n - \"8000\"\n            volumes\n                卷挂载路径设置\n                可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n                volumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n            volumes_from\n                从另一个服务或容器挂载它的所有卷\n                volumes_from:\n - service_name\n - container_name\n            environment\n                设置环境变量。你可以使用数组或字典两种格式。\n                只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n                environment:\n  RACK_ENV: development\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n            env_file\n                从文件中获取环境变量，可以为单独的文件路径或列表。\n                如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n                如果有变量名称与 environment 指令冲突，则以后者为准。\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n                # common.env: Set Rails/Rack environment\nRACK_ENV=development\n            extends\n                基于已有的服务进行扩展\n                例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n                # common.yml\nwebapp:\n  build: ./webapp\n  environment:\n    - DEBUG=false\n    - SEND_EMAILS=false\n                编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n                # development.yml\nweb:\n  extends:\n    file: common.yml\n    service: webapp\n  ports:\n    - \"8000:8000\"\n  links:\n    - db\n  environment:\n    - DEBUG=true\ndb:\n  image: postgres\n                后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\n            labels\n            container_name\n            log driver\n            net\n                设置网络模式\n                使用和 docker client 的 --net 参数一样的值\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                跟主机系统共享进程命名空间\n                打开该选项的容器可以相互通过进程 ID 来访问和操作\n                pid: \"host\"\n            dns\n                配置 DNS 服务器\n                可以是一个值，也可以是一个列表\n                dns: 8.8.8.8\n- dns:\n\n  - 8.8.8.8\n  - 9.9.9.9\n            cap_add, cap_drop\n                添加或放弃容器的 Linux 能力（Capabiliity）\n                cap_add:\n  - ALL\n\n- cap_drop:\n\n  - NET_ADMIN\n  - SYS_ADMIN\n            dns_search\n                配置 DNS 搜索域\n                可以是一个值，也可以是一个列表\n                dns_search: example.com\n- dns_search:\n\n  - domain1.example.com\n  - domain2.example.com\n            devices\n            security_opt\n            working_dir, entrypoint, user, hostname, domainname, \nmac_address, mem_limit, memswap_limit, privileged, \nrestart, stdin_open, tty, cpu_shares, cpuset, \nread_only, volume_driver\n                这些都是和 docker run 支持的选项类似\n                cpu_shares: 73\n\nworking_dir: /code\nentrypoint: /code/entrypoint.sh\nuser: postgresql\n\nhostname: foo\ndomainname: foo.com\n\nmem_limit: 1000000000\nprivileged: true\n\nrestart: always\n\nstdin_open: true\ntty: true\n        keyword\n            build\n                Path to a directory containing a Dockerfile.\n                build: /path/to/build/dir\n            cap_add, cap_drop\n                Add or drop container capabilities. See man 7 capabilities for a full list.\n                cap_add:\n  - ALL\n\ncap_drop:\n  - NET_ADMIN\n  - SYS_ADMIN\n            command\n                Override the default command.\n                command: bundle exec thin -p 3000\n            cgroup_parent\n                Specify an optional parent cgroup for the container.\n                cgroup_parent: m-executor-abcd\n            container_name\n                Specify a custom container name, rather than a generated default name.\n                container_name: my-web-container\n            devices\n                List of device mappings. Uses the same format as the --device docker client create option.\n                devices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n            dns\n                Custom DNS servers. Can be a single value or a list.\n                dns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n            dns_search\n                Custom DNS search domains. Can be a single value or a list.\n                dns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n            dockerfile\n                Alternate Dockerfile.\nCompose will use an alternate file to build with.\n                dockerfile: Dockerfile-alternate\n            env_file\n                Add environment variables from a file. Can be a single value or a list.\nIf you have specified a Compose file with docker-compose -f FILE, \npaths in env_file are relative to the directory that file is in.\nEnvironment variables specified in environment override these values.\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                Compose expects each line in an env file to be in VAR=VAL format.\n Lines beginning with # (i.e. comments) are ignored, as are blank lines.\n\n# Set Rails/Rack environment\nRACK_ENV=development\n            environment\n                Add environment variables. You can use either an array or a dictionary.\n                Any boolean values; true, false, yes no, need to be enclosed in quotes\n to ensure they are not converted to True or False by the YML parser.\n                Environment variables with only a key are resolved to their values on the machine Compose is running on, \nwhich can be helpful for secret or host-specific values.\n                environment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n            expose\n                Expose ports without publishing them to the host machine - \nthey’ll only be accessible to linked services. Only the internal port can be specified.\n                expose:\n - \"3000\"\n - \"8000\"\n            extends\n                Extend another service, in the current file or another, optionally overriding configuration.\nYou can use extends on any service together with other configuration keys.\n The extends value must be a dictionary defined with a required service and an optional file key.\n                extends:\n  file: common.yml\n  service: webapp\n            external_links\n                Link to containers started outside this docker-compose.yml or even outside of Compose,\n especially for containers that provide shared or common services. \nexternal_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n                Add hostname mappings. Use the same values as the docker client --add-host parameter.\n                extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n                An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n\n162.242.195.82  somehost\n50.31.209.229   otherhost\n            image\n                Tag or partial image ID. Can be local or remote - \nCompose will attempt to pull if it doesn’t exist locally.\n                image: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n            labels\n                Add metadata to containers using Docker labels. You can use either an array or a dictionary.\nIt’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.\n                labels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n            links\n                Link to containers in another service. \nEither specify both the service name and the link alias (SERVICE:ALIAS), \nor just the service name (which will also be used for the alias).\n                links:\n - db\n - db:database\n - redis\n                An entry with the alias’ name will be created in /etc/hosts \ninside containers for this service, e.g:\n\n172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            log_driver\n                Specify a logging driver for the service’s containers, as with the --log-driver option for docker run\n                The default value is json-file.\nNote: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs. \nUsing any other driver will not print any logs.\n                log_driver: \"json-file\"\nlog_driver: \"syslog\"\nlog_driver: \"none\"\n            log_opt\n                Specify logging options with log_opt for the logging driver, as with the --log-opt option for docker run.\n                log_driver: \"syslog\"\nlog_opt:\n  syslog-address: \"tcp://192.168.0.42:123\"\n            net\n                Networking mode. Use the same values as the docker client --net parameter.\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                pid: \"host\"\n                Sets the PID mode to the host PID mode. \nThis turns on sharing between container and the host oprating system the PID address space.\n Containers launched with this flag will be able to access and manipulate other containers\n in the bare-metal machine’s namespace and vise-versa.\n            ports\n                Expose ports. Either specify both ports (HOST:CONTAINER), \nor just the container port (a random host port will be chosen).\n                Note: When mapping ports in the HOST:CONTAINER format, \nyou may experience erroneous results when using a container port lower than 60, \nbecause YAML will parse numbers in the format xx:yy as sexagesimal (base 60). \nFor this reason, we recommend always explicitly specifying your port mappings as strings.\n                ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n            security_opt\n                Override the default labeling scheme for each container.\n                security_opt:\n    - label:user:USER\n    - label:role:ROLE\n            ulimits\n                Override the default ulimits for a container. \nYou can either specify a single limit as an integer or soft/hard limits as a mapping.\n                ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n            volumes, volume_driver\n                Mount paths as volumes, optionally specifying a path on the host machine \n(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).\n                volumes:\n - /var/lib/mysql\n - ./cache:/tmp/cache\n - ~/configs:/etc/configs/:ro\n                You can mount a relative path on the host, which will expand relative to the director\ny of the Compose configuration file being used. Relative paths should always begin with . or ...\n                If you use a volume name (instead of a volume path), you may also specify a volume_driver.\nvolume_driver: mydriver\nNote: No path expansion will be done if you have also specified a volume_driver.\n            volumes_from\n                Mount all of the volumes from another service or container, \noptionally specifying read-only access(ro) or read-write(rw).\n                volumes_from:\n - service_name\n - container_name\n - service_name:rw\n            cpu_shares, cpuset, domainname, entrypoint, hostname, \nipc, mac_address, mem_limit, memswap_limit, privileged, \nread_only, restart, stdin_open, tty, user, working_dir\n                Each of these is a single value, analogous to its docker run counterpart.\n                cpu_shares: 73\ncpuset: 0,1\n\nentrypoint: /code/entrypoint.sh\nuser: postgresql\nworking_dir: /code\n\ndomainname: foo.com\nhostname: foo\nipc: host\nmac_address: 02:42:ac:11:65:43\n\nmem_limit: 1000000000\nmemswap_limit: 2000000000\nprivileged: true\n\nrestart: always\n\nread_only: true\nstdin_open: true\ntty: true","source":"_posts/docker-compose-file-tmp.md","raw":"    ```\n        containers:\n        web:\n         build: .\n         command: python app.py\n         ports:\n         - \"5000:5000\"\n         volumes:\n         - .:/code\n         links:\n         - redis\n         environment:\n         - PYTHONUNBUFFERED=1\n        redis:\n         image: redis:latest\n         command: redis-server --appendonly yes\n     ```\n\n     ```\n        wiki2:\n        image: 'nickstenning/mediawiki'\n        ports:\n            - \"8880:80\"\n        links:\n            - db:database\n        volumes:\n            - /data/wiki2:/data\n\n        db:\n        image: \"mysql\"\n        expose:\n            - \"3306\"\n        environment:\n            - MYSQL_ROOT_PASSWORD=defaultpass\n\n    上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n    第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n    定义完成后，通过下面的命令来启动应用：\n\n    docker-compose up\n\n    links指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n\n# Variable substitution\n            \nBoth $VARIABLE and ${VARIABLE} syntax are supported. \nExtended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.\n            db:\n  image: \"postgres:${POSTGRES_VERSION}\"\n            web:\n  build: .\n  command: \"$$VAR_NOT_INTERPOLATED_BY_COMPOSE\"\n        \n# install\n\n    curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk 'NR==1{print $NF}')/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n        doc\n            docker-compose.yml reference\n                https://docs.docker.com/compose/yml/\n        keywords\n            image\n                指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n                image: ubuntu\n            build\n                指定 Dockerfile 所在文件夹的路径\n                Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n                build: /path/to/build/dir\n            dockerfile\n            command\n                覆盖容器启动后默认执行的命令。\n                command: bundle exec thin -p 3000\n            links\n                链接到其它服务中的容器\n                使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                    links:\n - db\n - db:database\n - redis\n                使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n                    172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            external_links\n                链接到 docker-compose.yml 外部的容器\n                甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n            ports\n                暴露端口信息\n                使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n                ports:\n - \"3000\"\n - \"8000:8000\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n            expose\n                暴露端口，但不映射到宿主机，只被连接的服务访问\n                仅可以指定内部端口为参数\n                expose:\n - \"3000\"\n - \"8000\"\n            volumes\n                卷挂载路径设置\n                可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n                volumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n            volumes_from\n                从另一个服务或容器挂载它的所有卷\n                volumes_from:\n - service_name\n - container_name\n            environment\n                设置环境变量。你可以使用数组或字典两种格式。\n                只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n                environment:\n  RACK_ENV: development\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n            env_file\n                从文件中获取环境变量，可以为单独的文件路径或列表。\n                如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n                如果有变量名称与 environment 指令冲突，则以后者为准。\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n                # common.env: Set Rails/Rack environment\nRACK_ENV=development\n            extends\n                基于已有的服务进行扩展\n                例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n                # common.yml\nwebapp:\n  build: ./webapp\n  environment:\n    - DEBUG=false\n    - SEND_EMAILS=false\n                编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n                # development.yml\nweb:\n  extends:\n    file: common.yml\n    service: webapp\n  ports:\n    - \"8000:8000\"\n  links:\n    - db\n  environment:\n    - DEBUG=true\ndb:\n  image: postgres\n                后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\n            labels\n            container_name\n            log driver\n            net\n                设置网络模式\n                使用和 docker client 的 --net 参数一样的值\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                跟主机系统共享进程命名空间\n                打开该选项的容器可以相互通过进程 ID 来访问和操作\n                pid: \"host\"\n            dns\n                配置 DNS 服务器\n                可以是一个值，也可以是一个列表\n                dns: 8.8.8.8\n- dns:\n\n  - 8.8.8.8\n  - 9.9.9.9\n            cap_add, cap_drop\n                添加或放弃容器的 Linux 能力（Capabiliity）\n                cap_add:\n  - ALL\n\n- cap_drop:\n\n  - NET_ADMIN\n  - SYS_ADMIN\n            dns_search\n                配置 DNS 搜索域\n                可以是一个值，也可以是一个列表\n                dns_search: example.com\n- dns_search:\n\n  - domain1.example.com\n  - domain2.example.com\n            devices\n            security_opt\n            working_dir, entrypoint, user, hostname, domainname, \nmac_address, mem_limit, memswap_limit, privileged, \nrestart, stdin_open, tty, cpu_shares, cpuset, \nread_only, volume_driver\n                这些都是和 docker run 支持的选项类似\n                cpu_shares: 73\n\nworking_dir: /code\nentrypoint: /code/entrypoint.sh\nuser: postgresql\n\nhostname: foo\ndomainname: foo.com\n\nmem_limit: 1000000000\nprivileged: true\n\nrestart: always\n\nstdin_open: true\ntty: true\n        keyword\n            build\n                Path to a directory containing a Dockerfile.\n                build: /path/to/build/dir\n            cap_add, cap_drop\n                Add or drop container capabilities. See man 7 capabilities for a full list.\n                cap_add:\n  - ALL\n\ncap_drop:\n  - NET_ADMIN\n  - SYS_ADMIN\n            command\n                Override the default command.\n                command: bundle exec thin -p 3000\n            cgroup_parent\n                Specify an optional parent cgroup for the container.\n                cgroup_parent: m-executor-abcd\n            container_name\n                Specify a custom container name, rather than a generated default name.\n                container_name: my-web-container\n            devices\n                List of device mappings. Uses the same format as the --device docker client create option.\n                devices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n            dns\n                Custom DNS servers. Can be a single value or a list.\n                dns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n            dns_search\n                Custom DNS search domains. Can be a single value or a list.\n                dns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n            dockerfile\n                Alternate Dockerfile.\nCompose will use an alternate file to build with.\n                dockerfile: Dockerfile-alternate\n            env_file\n                Add environment variables from a file. Can be a single value or a list.\nIf you have specified a Compose file with docker-compose -f FILE, \npaths in env_file are relative to the directory that file is in.\nEnvironment variables specified in environment override these values.\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                Compose expects each line in an env file to be in VAR=VAL format.\n Lines beginning with # (i.e. comments) are ignored, as are blank lines.\n\n# Set Rails/Rack environment\nRACK_ENV=development\n            environment\n                Add environment variables. You can use either an array or a dictionary.\n                Any boolean values; true, false, yes no, need to be enclosed in quotes\n to ensure they are not converted to True or False by the YML parser.\n                Environment variables with only a key are resolved to their values on the machine Compose is running on, \nwhich can be helpful for secret or host-specific values.\n                environment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n            expose\n                Expose ports without publishing them to the host machine - \nthey’ll only be accessible to linked services. Only the internal port can be specified.\n                expose:\n - \"3000\"\n - \"8000\"\n            extends\n                Extend another service, in the current file or another, optionally overriding configuration.\nYou can use extends on any service together with other configuration keys.\n The extends value must be a dictionary defined with a required service and an optional file key.\n                extends:\n  file: common.yml\n  service: webapp\n            external_links\n                Link to containers started outside this docker-compose.yml or even outside of Compose,\n especially for containers that provide shared or common services. \nexternal_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n                Add hostname mappings. Use the same values as the docker client --add-host parameter.\n                extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n                An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n\n162.242.195.82  somehost\n50.31.209.229   otherhost\n            image\n                Tag or partial image ID. Can be local or remote - \nCompose will attempt to pull if it doesn’t exist locally.\n                image: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n            labels\n                Add metadata to containers using Docker labels. You can use either an array or a dictionary.\nIt’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.\n                labels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n            links\n                Link to containers in another service. \nEither specify both the service name and the link alias (SERVICE:ALIAS), \nor just the service name (which will also be used for the alias).\n                links:\n - db\n - db:database\n - redis\n                An entry with the alias’ name will be created in /etc/hosts \ninside containers for this service, e.g:\n\n172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            log_driver\n                Specify a logging driver for the service’s containers, as with the --log-driver option for docker run\n                The default value is json-file.\nNote: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs. \nUsing any other driver will not print any logs.\n                log_driver: \"json-file\"\nlog_driver: \"syslog\"\nlog_driver: \"none\"\n            log_opt\n                Specify logging options with log_opt for the logging driver, as with the --log-opt option for docker run.\n                log_driver: \"syslog\"\nlog_opt:\n  syslog-address: \"tcp://192.168.0.42:123\"\n            net\n                Networking mode. Use the same values as the docker client --net parameter.\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                pid: \"host\"\n                Sets the PID mode to the host PID mode. \nThis turns on sharing between container and the host oprating system the PID address space.\n Containers launched with this flag will be able to access and manipulate other containers\n in the bare-metal machine’s namespace and vise-versa.\n            ports\n                Expose ports. Either specify both ports (HOST:CONTAINER), \nor just the container port (a random host port will be chosen).\n                Note: When mapping ports in the HOST:CONTAINER format, \nyou may experience erroneous results when using a container port lower than 60, \nbecause YAML will parse numbers in the format xx:yy as sexagesimal (base 60). \nFor this reason, we recommend always explicitly specifying your port mappings as strings.\n                ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n            security_opt\n                Override the default labeling scheme for each container.\n                security_opt:\n    - label:user:USER\n    - label:role:ROLE\n            ulimits\n                Override the default ulimits for a container. \nYou can either specify a single limit as an integer or soft/hard limits as a mapping.\n                ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n            volumes, volume_driver\n                Mount paths as volumes, optionally specifying a path on the host machine \n(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).\n                volumes:\n - /var/lib/mysql\n - ./cache:/tmp/cache\n - ~/configs:/etc/configs/:ro\n                You can mount a relative path on the host, which will expand relative to the director\ny of the Compose configuration file being used. Relative paths should always begin with . or ...\n                If you use a volume name (instead of a volume path), you may also specify a volume_driver.\nvolume_driver: mydriver\nNote: No path expansion will be done if you have also specified a volume_driver.\n            volumes_from\n                Mount all of the volumes from another service or container, \noptionally specifying read-only access(ro) or read-write(rw).\n                volumes_from:\n - service_name\n - container_name\n - service_name:rw\n            cpu_shares, cpuset, domainname, entrypoint, hostname, \nipc, mac_address, mem_limit, memswap_limit, privileged, \nread_only, restart, stdin_open, tty, user, working_dir\n                Each of these is a single value, analogous to its docker run counterpart.\n                cpu_shares: 73\ncpuset: 0,1\n\nentrypoint: /code/entrypoint.sh\nuser: postgresql\nworking_dir: /code\n\ndomainname: foo.com\nhostname: foo\nipc: host\nmac_address: 02:42:ac:11:65:43\n\nmem_limit: 1000000000\nmemswap_limit: 2000000000\nprivileged: true\n\nrestart: always\n\nread_only: true\nstdin_open: true\ntty: true","slug":"docker-compose-file-tmp","published":1,"date":"2017-01-18T07:23:48.000Z","updated":"2017-01-18T07:23:48.000Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38p001n21sv7kz4kcuv","content":"<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">containers:</div><div class=\"line\">web:</div><div class=\"line\"> build: .</div><div class=\"line\"> command: python app.py</div><div class=\"line\"> ports:</div><div class=\"line\"> - &quot;5000:5000&quot;</div><div class=\"line\"> volumes:</div><div class=\"line\"> - .:/code</div><div class=\"line\"> links:</div><div class=\"line\"> - redis</div><div class=\"line\"> environment:</div><div class=\"line\"> - PYTHONUNBUFFERED=1</div><div class=\"line\">redis:</div><div class=\"line\"> image: redis:latest</div><div class=\"line\"> command: redis-server --appendonly yes</div></pre></td></tr></table></figure>\n\n ```\n    wiki2:\n    image: &apos;nickstenning/mediawiki&apos;\n    ports:\n        - &quot;8880:80&quot;\n    links:\n        - db:database\n    volumes:\n        - /data/wiki2:/data\n\n    db:\n    image: &quot;mysql&quot;\n    expose:\n        - &quot;3306&quot;\n    environment:\n        - MYSQL_ROOT_PASSWORD=defaultpass\n\n上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n定义完成后，通过下面的命令来启动应用：\n\ndocker-compose up\n\nlinks指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n</code></pre><h1 id=\"Variable-substitution\"><a href=\"#Variable-substitution\" class=\"headerlink\" title=\"Variable substitution\"></a>Variable substitution</h1><p>Both $VARIABLE and ${VARIABLE} syntax are supported.<br>Extended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.<br>            db:<br>  image: “postgres:${POSTGRES_VERSION}”<br>            web:<br>  build: .<br>  command: “$$VAR_NOT_INTERPOLATED_BY_COMPOSE”</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk &apos;NR==1{print $NF}&apos;)/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose\n    doc\n        docker-compose.yml reference\n            https://docs.docker.com/compose/yml/\n    keywords\n        image\n            指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n            image: ubuntu\n        build\n            指定 Dockerfile 所在文件夹的路径\n            Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n            build: /path/to/build/dir\n        dockerfile\n        command\n            覆盖容器启动后默认执行的命令。\n            command: bundle exec thin -p 3000\n        links\n            链接到其它服务中的容器\n            使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n    172.17.2.186  db\n</code></pre>172.17.2.186  database<br>172.17.2.187  redis<pre><code>external_links\n    链接到 docker-compose.yml 外部的容器\n    甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n    external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\nports\n    暴露端口信息\n    使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n    ports:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000:8000”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”<pre><code>expose\n    暴露端口，但不映射到宿主机，只被连接的服务访问\n    仅可以指定内部端口为参数\n    expose:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000”<pre><code>volumes\n    卷挂载路径设置\n    可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n    volumes:\n</code></pre></li>\n<li>/var/lib/mysql</li>\n<li>cache/:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>volumes_from\n    从另一个服务或容器挂载它的所有卷\n    volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name<pre><code>environment\n    设置环境变量。你可以使用数组或字典两种格式。\n    只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n    environment:\n</code></pre>RACK_ENV: development<br>SESSION_SECRET:</li>\n</ul>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SESSION_SECRET<pre><code>env_file\n    从文件中获取环境变量，可以为单独的文件路径或列表。\n    如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n    如果有变量名称与 environment 指令冲突，则以后者为准。\n    env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li><p>/opt/secrets.env</p>\n<pre><code>环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n# common.env: Set Rails/Rack environment\n</code></pre><p>RACK_ENV=development</p>\n<pre><code>extends\n    基于已有的服务进行扩展\n    例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n    # common.yml\n</code></pre><p>webapp:<br>build: ./webapp<br>environment:</p>\n<ul>\n<li>DEBUG=false</li>\n<li>SEND_EMAILS=false<pre><code>编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n# development.yml\n</code></pre>web:<br>extends:<br>file: common.yml<br>service: webapp<br>ports:</li>\n<li>“8000:8000”<br>links:</li>\n<li>db<br>environment:</li>\n<li>DEBUG=true<br>db:<br>image: postgres<pre><code>    后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\nlabels\ncontainer_name\nlog driver\nnet\n    设置网络模式\n    使用和 docker client 的 --net 参数一样的值\n    net: &quot;bridge&quot;\n</code></pre>net: “none”<br>net: “container:[name or id]”<br>net: “host”<pre><code>pid\n    跟主机系统共享进程命名空间\n    打开该选项的容器可以相互通过进程 ID 来访问和操作\n    pid: &quot;host&quot;\ndns\n    配置 DNS 服务器\n    可以是一个值，也可以是一个列表\n    dns: 8.8.8.8\n</code></pre></li>\n<li>dns:</li>\n</ul>\n</li>\n<li><p>8.8.8.8</p>\n</li>\n<li>9.9.9.9<pre><code>cap_add, cap_drop\n    添加或放弃容器的 Linux 能力（Capabiliity）\n    cap_add:\n</code></pre></li>\n<li>ALL</li>\n</ul>\n<ul>\n<li><p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>dns_search\n    配置 DNS 搜索域\n    可以是一个值，也可以是一个列表\n    dns_search: example.com\n</code></pre></li>\n</ul>\n</li>\n<li><p>dns_search:</p>\n<ul>\n<li>domain1.example.com</li>\n<li>domain2.example.com<pre><code>devices\nsecurity_opt\nworking_dir, entrypoint, user, hostname, domainname, \n</code></pre>mac_address, mem_limit, memswap_limit, privileged,<br>restart, stdin_open, tty, cpu_shares, cpuset,<br>read_only, volume_driver<pre><code>这些都是和 docker run 支持的选项类似\ncpu_shares: 73\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>working_dir: /code<br>entrypoint: /code/entrypoint.sh<br>user: postgresql</p>\n<p>hostname: foo<br>domainname: foo.com</p>\n<p>mem_limit: 1000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>stdin_open: true<br>tty: true<br>        keyword<br>            build<br>                Path to a directory containing a Dockerfile.<br>                build: /path/to/build/dir<br>            cap_add, cap_drop<br>                Add or drop container capabilities. See man 7 capabilities for a full list.<br>                cap_add:</p>\n<ul>\n<li>ALL</li>\n</ul>\n<p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>command\n    Override the default command.\n    command: bundle exec thin -p 3000\ncgroup_parent\n    Specify an optional parent cgroup for the container.\n    cgroup_parent: m-executor-abcd\ncontainer_name\n    Specify a custom container name, rather than a generated default name.\n    container_name: my-web-container\ndevices\n    List of device mappings. Uses the same format as the --device docker client create option.\n    devices:\n</code></pre></li>\n<li>“/dev/ttyUSB0:/dev/ttyUSB0”<pre><code>dns\n    Custom DNS servers. Can be a single value or a list.\n    dns: 8.8.8.8\n</code></pre>dns:</li>\n<li>8.8.8.8</li>\n<li>9.9.9.9<pre><code>dns_search\n    Custom DNS search domains. Can be a single value or a list.\n    dns_search: example.com\n</code></pre>dns_search:</li>\n<li>dc1.example.com</li>\n<li>dc2.example.com<pre><code>dockerfile\n    Alternate Dockerfile.\n</code></pre>Compose will use an alternate file to build with.<pre><code>    dockerfile: Dockerfile-alternate\nenv_file\n    Add environment variables from a file. Can be a single value or a list.\n</code></pre>If you have specified a Compose file with docker-compose -f FILE,<br>paths in env_file are relative to the directory that file is in.<br>Environment variables specified in environment override these values.<pre><code>env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li>/opt/secrets.env<pre><code>Compose expects each line in an env file to be in VAR=VAL format.\n</code></pre>Lines beginning with # (i.e. comments) are ignored, as are blank lines.</li>\n</ul>\n<h1 id=\"Set-Rails-Rack-environment\"><a href=\"#Set-Rails-Rack-environment\" class=\"headerlink\" title=\"Set Rails/Rack environment\"></a>Set Rails/Rack environment</h1><p>RACK_ENV=development<br>            environment<br>                Add environment variables. You can use either an array or a dictionary.<br>                Any boolean values; true, false, yes no, need to be enclosed in quotes<br> to ensure they are not converted to True or False by the YML parser.<br>                Environment variables with only a key are resolved to their values on the machine Compose is running on,<br>which can be helpful for secret or host-specific values.<br>                environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SHOW=true</li>\n<li>SESSION_SECRET<pre><code>expose\n    Expose ports without publishing them to the host machine - \n</code></pre>they’ll only be accessible to linked services. Only the internal port can be specified.<pre><code>expose:\n</code></pre><ul>\n<li>“3000”</li>\n<li>“8000”<pre><code>extends\n    Extend another service, in the current file or another, optionally overriding configuration.\n</code></pre>You can use extends on any service together with other configuration keys.<br>The extends value must be a dictionary defined with a required service and an optional file key.<pre><code>extends:\n</code></pre>file: common.yml<br>service: webapp<pre><code>external_links\n    Link to containers started outside this docker-compose.yml or even outside of Compose,\n</code></pre>especially for containers that provide shared or common services.<br>external_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).<pre><code>external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\n    Add hostname mappings. Use the same values as the docker client --add-host parameter.\n    extra_hosts:\n</code></pre></li>\n<li>“somehost:162.242.195.82”</li>\n<li>“otherhost:50.31.209.229”<pre><code>An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>162.242.195.82  somehost<br>50.31.209.229   otherhost<br>            image<br>                Tag or partial image ID. Can be local or remote -<br>Compose will attempt to pull if it doesn’t exist locally.<br>                image: ubuntu<br>image: orchardup/postgresql<br>image: a4bc65fd<br>            labels<br>                Add metadata to containers using Docker labels. You can use either an array or a dictionary.<br>It’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.<br>                labels:<br>  com.example.description: “Accounting webapp”<br>  com.example.department: “Finance”<br>  com.example.label-with-empty-value: “”</p>\n<p>labels:</p>\n<ul>\n<li>“com.example.description=Accounting webapp”</li>\n<li>“com.example.department=Finance”</li>\n<li>“com.example.label-with-empty-value”<pre><code>links\n    Link to containers in another service. \n</code></pre>Either specify both the service name and the link alias (SERVICE:ALIAS),<br>or just the service name (which will also be used for the alias).<pre><code>links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>An entry with the alias’ name will be created in /etc/hosts \n</code></pre>inside containers for this service, e.g:</li>\n</ul>\n</li>\n</ul>\n<p>172.17.2.186  db<br>172.17.2.186  database<br>172.17.2.187  redis<br>            log_driver<br>                Specify a logging driver for the service’s containers, as with the –log-driver option for docker run<br>                The default value is json-file.<br>Note: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs.<br>Using any other driver will not print any logs.<br>                log_driver: “json-file”<br>log_driver: “syslog”<br>log_driver: “none”<br>            log_opt<br>                Specify logging options with log_opt for the logging driver, as with the –log-opt option for docker run.<br>                log_driver: “syslog”<br>log_opt:<br>  syslog-address: “tcp://192.168.0.42:123”<br>            net<br>                Networking mode. Use the same values as the docker client –net parameter.<br>                net: “bridge”<br>net: “none”<br>net: “container:[name or id]”<br>net: “host”<br>            pid<br>                pid: “host”<br>                Sets the PID mode to the host PID mode.<br>This turns on sharing between container and the host oprating system the PID address space.<br> Containers launched with this flag will be able to access and manipulate other containers<br> in the bare-metal machine’s namespace and vise-versa.<br>            ports<br>                Expose ports. Either specify both ports (HOST:CONTAINER),<br>or just the container port (a random host port will be chosen).<br>                Note: When mapping ports in the HOST:CONTAINER format,<br>you may experience erroneous results when using a container port lower than 60,<br>because YAML will parse numbers in the format xx:yy as sexagesimal (base 60).<br>For this reason, we recommend always explicitly specifying your port mappings as strings.<br>                ports:</p>\n<ul>\n<li>“3000”</li>\n<li>“3000-3005”</li>\n<li>“8000:8000”</li>\n<li>“9090-9091:8080-8081”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”</li>\n<li>“127.0.0.1:5000-5010:5000-5010”<pre><code>security_opt\n    Override the default labeling scheme for each container.\n    security_opt:\n</code></pre><ul>\n<li>label:user:USER</li>\n<li>label:role:ROLE<pre><code>ulimits\n    Override the default ulimits for a container. \n</code></pre>You can either specify a single limit as an integer or soft/hard limits as a mapping.<pre><code>ulimits:\n</code></pre>nproc: 65535<br>nofile:<br>soft: 20000<br>hard: 40000<pre><code>volumes, volume_driver\n    Mount paths as volumes, optionally specifying a path on the host machine \n</code></pre>(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).<pre><code>volumes:\n</code></pre></li>\n</ul>\n</li>\n<li>/var/lib/mysql</li>\n<li>./cache:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>You can mount a relative path on the host, which will expand relative to the director\n</code></pre>y of the Compose configuration file being used. Relative paths should always begin with . or …<pre><code>If you use a volume name (instead of a volume path), you may also specify a volume_driver.\n</code></pre>volume_driver: mydriver<br>Note: No path expansion will be done if you have also specified a volume_driver.<pre><code>volumes_from\n    Mount all of the volumes from another service or container, \n</code></pre>optionally specifying read-only access(ro) or read-write(rw).<pre><code>volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name</li>\n<li>service_name:rw<pre><code>cpu_shares, cpuset, domainname, entrypoint, hostname, \n</code></pre>ipc, mac_address, mem_limit, memswap_limit, privileged,<br>read_only, restart, stdin_open, tty, user, working_dir<pre><code>Each of these is a single value, analogous to its docker run counterpart.\ncpu_shares: 73\n</code></pre>cpuset: 0,1</li>\n</ul>\n<p>entrypoint: /code/entrypoint.sh<br>user: postgresql<br>working_dir: /code</p>\n<p>domainname: foo.com<br>hostname: foo<br>ipc: host<br>mac_address: 02:42:ac:11:65:43</p>\n<p>mem_limit: 1000000000<br>memswap_limit: 2000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>read_only: true<br>stdin_open: true<br>tty: true</p>\n","excerpt":"","more":"<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">containers:</div><div class=\"line\">web:</div><div class=\"line\"> build: .</div><div class=\"line\"> command: python app.py</div><div class=\"line\"> ports:</div><div class=\"line\"> - &quot;5000:5000&quot;</div><div class=\"line\"> volumes:</div><div class=\"line\"> - .:/code</div><div class=\"line\"> links:</div><div class=\"line\"> - redis</div><div class=\"line\"> environment:</div><div class=\"line\"> - PYTHONUNBUFFERED=1</div><div class=\"line\">redis:</div><div class=\"line\"> image: redis:latest</div><div class=\"line\"> command: redis-server --appendonly yes</div></pre></td></tr></table></figure>\n\n ```\n    wiki2:\n    image: &apos;nickstenning/mediawiki&apos;\n    ports:\n        - &quot;8880:80&quot;\n    links:\n        - db:database\n    volumes:\n        - /data/wiki2:/data\n\n    db:\n    image: &quot;mysql&quot;\n    expose:\n        - &quot;3306&quot;\n    environment:\n        - MYSQL_ROOT_PASSWORD=defaultpass\n\n上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n定义完成后，通过下面的命令来启动应用：\n\ndocker-compose up\n\nlinks指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n</code></pre><h1 id=\"Variable-substitution\"><a href=\"#Variable-substitution\" class=\"headerlink\" title=\"Variable substitution\"></a>Variable substitution</h1><p>Both $VARIABLE and ${VARIABLE} syntax are supported.<br>Extended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.<br>            db:<br>  image: “postgres:${POSTGRES_VERSION}”<br>            web:<br>  build: .<br>  command: “$$VAR_NOT_INTERPOLATED_BY_COMPOSE”</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk &apos;NR==1{print $NF}&apos;)/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose\n    doc\n        docker-compose.yml reference\n            https://docs.docker.com/compose/yml/\n    keywords\n        image\n            指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n            image: ubuntu\n        build\n            指定 Dockerfile 所在文件夹的路径\n            Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n            build: /path/to/build/dir\n        dockerfile\n        command\n            覆盖容器启动后默认执行的命令。\n            command: bundle exec thin -p 3000\n        links\n            链接到其它服务中的容器\n            使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n    172.17.2.186  db\n</code></pre>172.17.2.186  database<br>172.17.2.187  redis<pre><code>external_links\n    链接到 docker-compose.yml 外部的容器\n    甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n    external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\nports\n    暴露端口信息\n    使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n    ports:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000:8000”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”<pre><code>expose\n    暴露端口，但不映射到宿主机，只被连接的服务访问\n    仅可以指定内部端口为参数\n    expose:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000”<pre><code>volumes\n    卷挂载路径设置\n    可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n    volumes:\n</code></pre></li>\n<li>/var/lib/mysql</li>\n<li>cache/:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>volumes_from\n    从另一个服务或容器挂载它的所有卷\n    volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name<pre><code>environment\n    设置环境变量。你可以使用数组或字典两种格式。\n    只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n    environment:\n</code></pre>RACK_ENV: development<br>SESSION_SECRET:</li>\n</ul>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SESSION_SECRET<pre><code>env_file\n    从文件中获取环境变量，可以为单独的文件路径或列表。\n    如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n    如果有变量名称与 environment 指令冲突，则以后者为准。\n    env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li><p>/opt/secrets.env</p>\n<pre><code>环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n# common.env: Set Rails/Rack environment\n</code></pre><p>RACK_ENV=development</p>\n<pre><code>extends\n    基于已有的服务进行扩展\n    例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n    # common.yml\n</code></pre><p>webapp:<br>build: ./webapp<br>environment:</p>\n<ul>\n<li>DEBUG=false</li>\n<li>SEND_EMAILS=false<pre><code>编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n# development.yml\n</code></pre>web:<br>extends:<br>file: common.yml<br>service: webapp<br>ports:</li>\n<li>“8000:8000”<br>links:</li>\n<li>db<br>environment:</li>\n<li>DEBUG=true<br>db:<br>image: postgres<pre><code>    后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\nlabels\ncontainer_name\nlog driver\nnet\n    设置网络模式\n    使用和 docker client 的 --net 参数一样的值\n    net: &quot;bridge&quot;\n</code></pre>net: “none”<br>net: “container:[name or id]”<br>net: “host”<pre><code>pid\n    跟主机系统共享进程命名空间\n    打开该选项的容器可以相互通过进程 ID 来访问和操作\n    pid: &quot;host&quot;\ndns\n    配置 DNS 服务器\n    可以是一个值，也可以是一个列表\n    dns: 8.8.8.8\n</code></pre></li>\n<li>dns:</li>\n</ul>\n</li>\n<li><p>8.8.8.8</p>\n</li>\n<li>9.9.9.9<pre><code>cap_add, cap_drop\n    添加或放弃容器的 Linux 能力（Capabiliity）\n    cap_add:\n</code></pre></li>\n<li>ALL</li>\n</ul>\n<ul>\n<li><p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>dns_search\n    配置 DNS 搜索域\n    可以是一个值，也可以是一个列表\n    dns_search: example.com\n</code></pre></li>\n</ul>\n</li>\n<li><p>dns_search:</p>\n<ul>\n<li>domain1.example.com</li>\n<li>domain2.example.com<pre><code>devices\nsecurity_opt\nworking_dir, entrypoint, user, hostname, domainname, \n</code></pre>mac_address, mem_limit, memswap_limit, privileged,<br>restart, stdin_open, tty, cpu_shares, cpuset,<br>read_only, volume_driver<pre><code>这些都是和 docker run 支持的选项类似\ncpu_shares: 73\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>working_dir: /code<br>entrypoint: /code/entrypoint.sh<br>user: postgresql</p>\n<p>hostname: foo<br>domainname: foo.com</p>\n<p>mem_limit: 1000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>stdin_open: true<br>tty: true<br>        keyword<br>            build<br>                Path to a directory containing a Dockerfile.<br>                build: /path/to/build/dir<br>            cap_add, cap_drop<br>                Add or drop container capabilities. See man 7 capabilities for a full list.<br>                cap_add:</p>\n<ul>\n<li>ALL</li>\n</ul>\n<p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>command\n    Override the default command.\n    command: bundle exec thin -p 3000\ncgroup_parent\n    Specify an optional parent cgroup for the container.\n    cgroup_parent: m-executor-abcd\ncontainer_name\n    Specify a custom container name, rather than a generated default name.\n    container_name: my-web-container\ndevices\n    List of device mappings. Uses the same format as the --device docker client create option.\n    devices:\n</code></pre></li>\n<li>“/dev/ttyUSB0:/dev/ttyUSB0”<pre><code>dns\n    Custom DNS servers. Can be a single value or a list.\n    dns: 8.8.8.8\n</code></pre>dns:</li>\n<li>8.8.8.8</li>\n<li>9.9.9.9<pre><code>dns_search\n    Custom DNS search domains. Can be a single value or a list.\n    dns_search: example.com\n</code></pre>dns_search:</li>\n<li>dc1.example.com</li>\n<li>dc2.example.com<pre><code>dockerfile\n    Alternate Dockerfile.\n</code></pre>Compose will use an alternate file to build with.<pre><code>    dockerfile: Dockerfile-alternate\nenv_file\n    Add environment variables from a file. Can be a single value or a list.\n</code></pre>If you have specified a Compose file with docker-compose -f FILE,<br>paths in env_file are relative to the directory that file is in.<br>Environment variables specified in environment override these values.<pre><code>env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li>/opt/secrets.env<pre><code>Compose expects each line in an env file to be in VAR=VAL format.\n</code></pre>Lines beginning with # (i.e. comments) are ignored, as are blank lines.</li>\n</ul>\n<h1 id=\"Set-Rails-Rack-environment\"><a href=\"#Set-Rails-Rack-environment\" class=\"headerlink\" title=\"Set Rails/Rack environment\"></a>Set Rails/Rack environment</h1><p>RACK_ENV=development<br>            environment<br>                Add environment variables. You can use either an array or a dictionary.<br>                Any boolean values; true, false, yes no, need to be enclosed in quotes<br> to ensure they are not converted to True or False by the YML parser.<br>                Environment variables with only a key are resolved to their values on the machine Compose is running on,<br>which can be helpful for secret or host-specific values.<br>                environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SHOW=true</li>\n<li>SESSION_SECRET<pre><code>expose\n    Expose ports without publishing them to the host machine - \n</code></pre>they’ll only be accessible to linked services. Only the internal port can be specified.<pre><code>expose:\n</code></pre><ul>\n<li>“3000”</li>\n<li>“8000”<pre><code>extends\n    Extend another service, in the current file or another, optionally overriding configuration.\n</code></pre>You can use extends on any service together with other configuration keys.<br>The extends value must be a dictionary defined with a required service and an optional file key.<pre><code>extends:\n</code></pre>file: common.yml<br>service: webapp<pre><code>external_links\n    Link to containers started outside this docker-compose.yml or even outside of Compose,\n</code></pre>especially for containers that provide shared or common services.<br>external_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).<pre><code>external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\n    Add hostname mappings. Use the same values as the docker client --add-host parameter.\n    extra_hosts:\n</code></pre></li>\n<li>“somehost:162.242.195.82”</li>\n<li>“otherhost:50.31.209.229”<pre><code>An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>162.242.195.82  somehost<br>50.31.209.229   otherhost<br>            image<br>                Tag or partial image ID. Can be local or remote -<br>Compose will attempt to pull if it doesn’t exist locally.<br>                image: ubuntu<br>image: orchardup/postgresql<br>image: a4bc65fd<br>            labels<br>                Add metadata to containers using Docker labels. You can use either an array or a dictionary.<br>It’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.<br>                labels:<br>  com.example.description: “Accounting webapp”<br>  com.example.department: “Finance”<br>  com.example.label-with-empty-value: “”</p>\n<p>labels:</p>\n<ul>\n<li>“com.example.description=Accounting webapp”</li>\n<li>“com.example.department=Finance”</li>\n<li>“com.example.label-with-empty-value”<pre><code>links\n    Link to containers in another service. \n</code></pre>Either specify both the service name and the link alias (SERVICE:ALIAS),<br>or just the service name (which will also be used for the alias).<pre><code>links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>An entry with the alias’ name will be created in /etc/hosts \n</code></pre>inside containers for this service, e.g:</li>\n</ul>\n</li>\n</ul>\n<p>172.17.2.186  db<br>172.17.2.186  database<br>172.17.2.187  redis<br>            log_driver<br>                Specify a logging driver for the service’s containers, as with the –log-driver option for docker run<br>                The default value is json-file.<br>Note: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs.<br>Using any other driver will not print any logs.<br>                log_driver: “json-file”<br>log_driver: “syslog”<br>log_driver: “none”<br>            log_opt<br>                Specify logging options with log_opt for the logging driver, as with the –log-opt option for docker run.<br>                log_driver: “syslog”<br>log_opt:<br>  syslog-address: “tcp://192.168.0.42:123”<br>            net<br>                Networking mode. Use the same values as the docker client –net parameter.<br>                net: “bridge”<br>net: “none”<br>net: “container:[name or id]”<br>net: “host”<br>            pid<br>                pid: “host”<br>                Sets the PID mode to the host PID mode.<br>This turns on sharing between container and the host oprating system the PID address space.<br> Containers launched with this flag will be able to access and manipulate other containers<br> in the bare-metal machine’s namespace and vise-versa.<br>            ports<br>                Expose ports. Either specify both ports (HOST:CONTAINER),<br>or just the container port (a random host port will be chosen).<br>                Note: When mapping ports in the HOST:CONTAINER format,<br>you may experience erroneous results when using a container port lower than 60,<br>because YAML will parse numbers in the format xx:yy as sexagesimal (base 60).<br>For this reason, we recommend always explicitly specifying your port mappings as strings.<br>                ports:</p>\n<ul>\n<li>“3000”</li>\n<li>“3000-3005”</li>\n<li>“8000:8000”</li>\n<li>“9090-9091:8080-8081”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”</li>\n<li>“127.0.0.1:5000-5010:5000-5010”<pre><code>security_opt\n    Override the default labeling scheme for each container.\n    security_opt:\n</code></pre><ul>\n<li>label:user:USER</li>\n<li>label:role:ROLE<pre><code>ulimits\n    Override the default ulimits for a container. \n</code></pre>You can either specify a single limit as an integer or soft/hard limits as a mapping.<pre><code>ulimits:\n</code></pre>nproc: 65535<br>nofile:<br>soft: 20000<br>hard: 40000<pre><code>volumes, volume_driver\n    Mount paths as volumes, optionally specifying a path on the host machine \n</code></pre>(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).<pre><code>volumes:\n</code></pre></li>\n</ul>\n</li>\n<li>/var/lib/mysql</li>\n<li>./cache:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>You can mount a relative path on the host, which will expand relative to the director\n</code></pre>y of the Compose configuration file being used. Relative paths should always begin with . or …<pre><code>If you use a volume name (instead of a volume path), you may also specify a volume_driver.\n</code></pre>volume_driver: mydriver<br>Note: No path expansion will be done if you have also specified a volume_driver.<pre><code>volumes_from\n    Mount all of the volumes from another service or container, \n</code></pre>optionally specifying read-only access(ro) or read-write(rw).<pre><code>volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name</li>\n<li>service_name:rw<pre><code>cpu_shares, cpuset, domainname, entrypoint, hostname, \n</code></pre>ipc, mac_address, mem_limit, memswap_limit, privileged,<br>read_only, restart, stdin_open, tty, user, working_dir<pre><code>Each of these is a single value, analogous to its docker run counterpart.\ncpu_shares: 73\n</code></pre>cpuset: 0,1</li>\n</ul>\n<p>entrypoint: /code/entrypoint.sh<br>user: postgresql<br>working_dir: /code</p>\n<p>domainname: foo.com<br>hostname: foo<br>ipc: host<br>mac_address: 02:42:ac:11:65:43</p>\n<p>mem_limit: 1000000000<br>memswap_limit: 2000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>read_only: true<br>stdin_open: true<br>tty: true</p>\n"},{"title":"docker core","_content":"\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","source":"_posts/docker-core.md","raw":"---\ntitle: docker core\ncategories:\n- docker\ntags:\n- core\n---\n\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","slug":"docker-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T06:28:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38t001s21svt0sqa4iq","content":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n"},{"title":"docker cookbook","_content":"\n# docker cookbook","source":"_posts/docker-cookbook.md","raw":"---\ntitle: docker cookbook\ncategories:\n- docker\ntags:\n- cookbook\n---\n\n# docker cookbook","slug":"docker-cookbook","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T10:51:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38y001u21svs7mji43n","content":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>","excerpt":"","more":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>"},{"title":"devops portal","_content":"\n# about\n\n","source":"_posts/devops--portal.md","raw":"---\ntitle: devops portal\ncategories:\n- devops\ntags:\n- portal\n---\n\n# about\n\n","slug":"devops--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T14:26:16.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm391001y21svngtxbmr2","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1>"},{"title":"dockerfile","_content":"\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n    \n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n    \n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n    \n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n    \n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n    \n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n    \n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n            \n        docker run -i -t --rm -p 80:80 nginx\n    \n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n    \n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n    \n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n    \n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n    \n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","source":"_posts/docker-dockerfile-detail.md","raw":"---\ntitle: dockerfile\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n    \n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n    \n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n    \n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n    \n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n    \n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n    \n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n            \n        docker run -i -t --rm -p 80:80 nginx\n    \n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n    \n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n    \n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n    \n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n    \n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","slug":"docker-dockerfile-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:46:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm394002121svo7zb0zb3","content":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</dest></src></p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\" target=\"_blank\" rel=\"external\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</dest></src></dest></src></p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\" target=\"_blank\" rel=\"external\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>","excerpt":"","more":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>"},{"title":"dockerhub","_content":"\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","source":"_posts/docker-dockerhub-detail.md","raw":"---\ntitle: dockerhub\ncategories:\n- docker\ntags:\n- detail\n---\n\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","slug":"docker-dockerhub-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:46:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm396002521svmkmo7uob","content":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\" target=\"_blank\" rel=\"external\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n"},{"title":"docker engine","_content":"\n## about\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","source":"_posts/docker-engine-detail.md","raw":"---\ntitle: docker engine\ncategories:\n- docker\ntags:\n- detail\n---\n\n## about\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","slug":"docker-engine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T12:20:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39a002821svvdj89we5","content":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\" target=\"_blank\" rel=\"external\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\" target=\"_blank\" rel=\"external\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"docker filesystem","_content":"\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","source":"_posts/docker-filesystem-core.md","raw":"---\ntitle: docker filesystem\ncategories:\n- docker\ntags:\n- core\n- filesystem\n---\n\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","slug":"docker-filesystem-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T07:30:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39c002b21svtsnp2crr","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\" target=\"_blank\" rel=\"external\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\" target=\"_blank\" rel=\"external\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\" target=\"_blank\" rel=\"external\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\" target=\"_blank\" rel=\"external\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n"},{"title":"docker image","_content":"\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","source":"_posts/docker-image-detail.md","raw":"---\ntitle: docker image\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","slug":"docker-image-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:50:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39e002f21svqd89nzvh","content":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\" target=\"_blank\" rel=\"external\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n"},{"title":"k8s core","_content":"\n#  core","source":"_posts/docker-k8s-core.md","raw":"---\ntitle: k8s core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-k8s-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39i002i21sv9msto5pk","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"docker machine","_content":"\n# about\n\n- link: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","source":"_posts/docker-machine-detail.md","raw":"---\ntitle: docker machine\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n- link: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","slug":"docker-machine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-18T05:50:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39m002n21svzvub4bz1","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/machine/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/machine/overview/</a></li>\n</ul>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</image></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/machine/overview/\">https://docs.docker.com/machine/overview/</a></li>\n</ul>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</p>\n"},{"title":"mesos core","_content":"\n#  core","source":"_posts/docker-mesos-core.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-mesos-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39p002q21svs69ki7jr","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"docker security","_content":"\n# about\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary","source":"_posts/docker-security-core.md","raw":"---\ntitle: docker security\ncategories:\n- docker\ntags:\n- core\n- security\n---\n\n# about\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary","slug":"docker-security-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:26:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39s002u21svylq4icy5","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\" target=\"_blank\" rel=\"external\">https://github.com/docker/notary</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\">https://github.com/docker/notary</a></p>\n"},{"title":"docker network","_content":"\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n    \n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n        \n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n            \n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n            \n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/docker-network-core.md","raw":"---\ntitle: docker network\ncategories:\n- docker\ntags:\n- core\n- network\n---\n\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n    \n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n        \n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n            \n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n            \n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"docker-network-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:25:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39x002x21svqqrk3m16","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n"},{"title":"docker storage","_content":"\n# about\n\n- data volumes 数据卷\n   \n    - create\n        \n        using dockerfile:\n            \n                VOLUME /var/lib/postgresql\n        \n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n    \n    - mount file\n        \n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n    \n    - mount folder\n        \n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n    \n    - mount local directory\n        \n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n    \n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n        \n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo \n            \n        restore:\n        \n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm","source":"_posts/docker-storage-core.md","raw":"---\ntitle: docker storage\ncategories:\n- docker\ntags:\n- core\n- storage\n---\n\n# about\n\n- data volumes 数据卷\n   \n    - create\n        \n        using dockerfile:\n            \n                VOLUME /var/lib/postgresql\n        \n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n    \n    - mount file\n        \n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n    \n    - mount folder\n        \n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n    \n    - mount local directory\n        \n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n    \n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n        \n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo \n            \n        restore:\n        \n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm","slug":"docker-storage-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T06:26:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3a2003221svf7toen7b","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo \n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\" target=\"_blank\" rel=\"external\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo \n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n"},{"title":"docker swarm portal","_content":"\n# about \n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","source":"_posts/docker-swarm-portal.md","raw":"---\ntitle: docker swarm portal\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about \n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","slug":"docker-swarm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T07:55:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ae003521svyblhs7lq","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\" target=\"_blank\" rel=\"external\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/swarm/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/swarm/overview/</a></li>\n</ul>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/swarm/overview/\">https://docs.docker.com/swarm/overview/</a></li>\n</ul>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>"},{"title":"mesos core","_content":"\n#  core","source":"_posts/docker-store-detail.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-store-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ai003921sv5f0f4edv","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"docker swarm","_content":"\n# about \n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local) \n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","source":"_posts/docker-swarm-detail.md","raw":"---\ntitle: docker swarm\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about \n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local) \n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","slug":"docker-swarm-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T07:30:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3al003d21svs6i4yavl","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local) </p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local) </p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n"},{"title":"openstack core","_content":"\n# about\n\n- link: https://github.com/docker/swarmkit\n\n# ","source":"_posts/docker-swarmkit-detail.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# about\n\n- link: https://github.com/docker/swarmkit\n\n# ","slug":"docker-swarmkit-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T08:04:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3an003h21sv7b0gw30l","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\" target=\"_blank\" rel=\"external\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p># </p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p># </p>\n"},{"title":"docker yaml file","_content":"\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML","source":"_posts/docker-yaml-detail.md","raw":"---\ntitle: docker yaml file\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML","slug":"docker-yaml-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:31:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ao003l21svrwyk2i8l","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\" target=\"_blank\" rel=\"external\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/YAML</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\">https://en.wikipedia.org/wiki/YAML</a></p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories:\n- tmp\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2017-01-11T09:26:51.000Z","updated":"2017-01-11T16:32:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3as003p21sv6fbp39yp","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"title":"docker swarm mode","_content":"\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","source":"_posts/docker-swarmnext-detail.md","raw":"---\ntitle: docker swarm mode\ncategories:\n- docker\ntags:\n- core\n- swarm\n---\n\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","slug":"docker-swarmnext-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3aw003s21svc9u1t1uj","content":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n"},{"title":"openstack core","_content":"\n# core","source":"_posts/iot-portal.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# core","slug":"iot-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ay003w21svpn73kvxk","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"cloud architect portal","_content":"\n# about cloud architect\n\n# resource\n\n","source":"_posts/jd-architect-portal.md","raw":"---\ntitle: cloud architect portal\ncategories:\n- cloud\ntags:\n- portal\n- architect\n---\n\n# about cloud architect\n\n# resource\n\n","slug":"jd-architect-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:48:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3b2004021svioptmeei","content":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>","excerpt":"","more":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>"},{"title":"python portal","_content":"\n# 123","source":"_posts/jd-fullstack-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"jd-fullstack-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3b4004421svqhoqjae5","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"cloud architect portal","_content":"\n# about cloud architect\n\n# resource\n\n","source":"_posts/jd--portal.md","raw":"---\ntitle: cloud architect portal\ncategories:\n- cloud\ntags:\n- portal\n- architect\n---\n\n# about cloud architect\n\n# resource\n\n","slug":"jd--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:48:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3b7004721sv1225ic1c","content":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>","excerpt":"","more":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>"},{"title":"cx portal","_content":"\n","source":"_posts/lang--portal.md","raw":"---\ntitle: cx portal\ncategories:\n- cx\ntags:\n- portal\n---\n\n","slug":"lang--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:31:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ba004c21svocwumvw5","content":"","excerpt":"","more":""},{"title":"cx portal","_content":"\n","source":"_posts/lang-cx-portal.md","raw":"---\ntitle: cx portal\ncategories:\n- cx\ntags:\n- portal\n---\n\n","slug":"lang-cx-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:31:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bc004f21svgewuytn4","content":"","excerpt":"","more":""},{"title":"java portal","_content":"\n#  portal","source":"_posts/lang-java-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"lang-java-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3be004k21svfutfhg13","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"nodejs core","_content":"\n\n        ","source":"_posts/lang-js-nodejs-core.md","raw":"---\ntitle: nodejs core\ncategories:\n- nodejs\ntags:\n- core\n---\n\n\n        ","slug":"lang-js-nodejs-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bg004n21sv7l1ddxrz","content":"","excerpt":"","more":""},{"title":"js portal","_content":"\n\n        ","source":"_posts/lang-js-portal.md","raw":"---\ntitle: js portal\ncategories:\n- js\ntags:\n- portal\n---\n\n\n        ","slug":"lang-js-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bj004r21svidlm72tf","content":"","excerpt":"","more":""},{"title":"django core","_content":"\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","source":"_posts/lang-python-django-core.md","raw":"---\ntitle: django core\ncategories:\n- python\ntags:\n- core\n- django\n---\n\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","slug":"lang-python-django-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-12T09:47:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bo004u21sve7kv8amn","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>"},{"title":"install django on mac","update":"2017-01-12T09:43:23.000Z","_content":"\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","source":"_posts/lang-python-django-install.md","raw":"---\ntitle: install django on mac\nupdate: 2017-01-12 17:43:23\ncategories:\n- python\ntags: \n- install\n- python\n- django\n- docker\n---\n\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","slug":"lang-python-django-install","published":1,"date":"2017-01-12T09:43:23.000Z","updated":"2017-01-12T09:49:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bs004x21svpb3i7i4k","content":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"python portal","_content":"\n# about\n\n# link\n\n    - official: \n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n","source":"_posts/lang-python-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# about\n\n# link\n\n    - official: \n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n","slug":"lang-python-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-12T09:42:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bw005221sv9xsoxqtn","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: \n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: \n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre>"},{"title":"microservice portal","_content":"\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","source":"_posts/microservice-portal.md","raw":"---\ntitle: microservice portal\ncategories:\n- microservice\ntags:\n- portal\n---\n\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","slug":"microservice-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:29:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bz005521sv6q6whiyk","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>"},{"title":"ruby on rails core","_content":"\n# 123","source":"_posts/lang-ruby-rails-core.md","raw":"---\ntitle: ruby on rails core\ncategories:\n- ruby\ntags:\n- core\n- rails\n---\n\n# 123","slug":"lang-ruby-rails-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:36:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3c2005921sv50p1s021","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"http core","_content":"\n#  core","source":"_posts/network-http-core.md","raw":"---\ntitle: http core\ncategories:\n- network\ntags:\n- core\n- http\n---\n\n#  core","slug":"network-http-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3c4005d21sv93q4gepl","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"network portal","_content":"\n# about\n\n\n# websocket\n","source":"_posts/network-portal.md","raw":"---\ntitle: network portal\ncategories:\n- network\ntags:\n- portal\n---\n\n# about\n\n\n# websocket\n","slug":"network-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:19.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3c7005h21svamd3gs1p","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>"},{"title":"linux portal","_content":"\n#  ","source":"_posts/system-linux-portal.md","raw":"---\ntitle: linux portal\ncategories:\n- linux\ntags:\n- portal\n---\n\n#  ","slug":"system-linux-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3cb005l21svlf0gc6a4","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>","excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>"}],"PostAsset":[],"PostCategory":[{"post_id":"ciypwm370000621sv5wqww7o5","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37f000c21sv2in622km"},{"post_id":"ciypwm36l000121sv4vi8ieps","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37k000h21sv2rz3wkhl"},{"post_id":"ciypwm373000721sve8sra0p8","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37n000k21svh1v35cu8"},{"post_id":"ciypwm36p000221svq1phbbk0","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37r000p21svxoyermy2"},{"post_id":"ciypwm36y000521svf4gipt4m","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37v000t21svxfxwbwdz"},{"post_id":"ciypwm37s000r21svd81wuly4","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm380000z21svrp0qp30g"},{"post_id":"ciypwm37b000b21svs9suqb35","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm386001421svy7qnw6ow"},{"post_id":"ciypwm37g000e21sv4w08jqwz","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm38e001921svvzpfj3wu"},{"post_id":"ciypwm37k000j21svx26uzv21","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm38k001g21sv7ribpger"},{"post_id":"ciypwm38b001821svqerj6t12","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm38o001l21svbbd3dx99"},{"post_id":"ciypwm37n000m21svlowv6kfh","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm38q001o21svkwebhqmb"},{"post_id":"ciypwm37v000u21svtfny6hag","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm390001v21sv5xbutbf0"},{"post_id":"ciypwm37z000y21sv7ku7mwqz","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm396002221sv9m6lwult"},{"post_id":"ciypwm381001121svtap4yfqx","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm39c002921svdo50m0mz"},{"post_id":"ciypwm39a002821svvdj89we5","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39h002g21svd8575f2r"},{"post_id":"ciypwm387001621sv0tyyvktn","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39l002l21sv3qvwb52d"},{"post_id":"ciypwm39c002b21svtsnp2crr","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39o002o21sv43kxy575"},{"post_id":"ciypwm39e002f21svqd89nzvh","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39s002s21svgisawsid"},{"post_id":"ciypwm38f001d21svebn6o2x0","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39x002v21sv6um6u400"},{"post_id":"ciypwm39i002i21sv9msto5pk","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3a2003021svvbegg3ew"},{"post_id":"ciypwm39m002n21svzvub4bz1","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ad003321svfkzibr6y"},{"post_id":"ciypwm38h001f21sv8w2usj1k","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ah003721svgctupmel"},{"post_id":"ciypwm39p002q21svs69ki7jr","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ak003b21svezheb98a"},{"post_id":"ciypwm39s002u21svylq4icy5","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3am003f21svyfb1bso1"},{"post_id":"ciypwm38l001k21svmq9vzpvt","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ao003j21svykzf8kik"},{"post_id":"ciypwm39x002x21svqqrk3m16","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3aq003n21sv57qqwtj4"},{"post_id":"ciypwm3a2003221svf7toen7b","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3av003q21sv4655peal"},{"post_id":"ciypwm38t001s21svt0sqa4iq","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ax003u21svml52h4kq"},{"post_id":"ciypwm3ae003521svyblhs7lq","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b1003y21svizhfs2et"},{"post_id":"ciypwm3ai003921sv5f0f4edv","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b4004221svxr2czuu1"},{"post_id":"ciypwm38y001u21svs7mji43n","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b6004521svmhcjtgvs"},{"post_id":"ciypwm3al003d21svs6i4yavl","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b9004a21sv6b26t5hr"},{"post_id":"ciypwm3an003h21sv7b0gw30l","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm3bc004d21sv2fk442tj"},{"post_id":"ciypwm391001y21svngtxbmr2","category_id":"ciypwm3am003e21svty5ib55t","_id":"ciypwm3be004h21sv0onu7yv3"},{"post_id":"ciypwm3ao003l21svrwyk2i8l","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bf004l21sv9592jem1"},{"post_id":"ciypwm394002121svo7zb0zb3","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bi004o21svkpp36ghb"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bo004s21sv0cjth70b"},{"post_id":"ciypwm3ay003w21svpn73kvxk","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm3br004v21svmuq2po06"},{"post_id":"ciypwm396002521svmkmo7uob","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bv005021svqe9z702f"},{"post_id":"ciypwm3b4004421svqhoqjae5","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3by005321svq3h76wzi"},{"post_id":"ciypwm3as003p21sv6fbp39yp","category_id":"ciypwm3b3004121svtkx85z4d","_id":"ciypwm3c1005721sv6rxllkw4"},{"post_id":"ciypwm3b2004021svioptmeei","category_id":"ciypwm3b9004921svnu7nqm17","_id":"ciypwm3c4005b21sv833gpiuv"},{"post_id":"ciypwm3be004k21svfutfhg13","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm3c7005f21sv8sqlhn2u"},{"post_id":"ciypwm3b7004721sv1225ic1c","category_id":"ciypwm3b9004921svnu7nqm17","_id":"ciypwm3cb005j21svgn3httg0"},{"post_id":"ciypwm3bo004u21sve7kv8amn","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3cd005n21svt30z6gya"},{"post_id":"ciypwm3ba004c21svocwumvw5","category_id":"ciypwm3bj004q21sve4wap485","_id":"ciypwm3cd005q21svuo138kpp"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3ce005s21sv4ndhbq84"},{"post_id":"ciypwm3bw005221sv9xsoxqtn","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3ce005w21svs54wwwg7"},{"post_id":"ciypwm3bc004f21svgewuytn4","category_id":"ciypwm3bj004q21sve4wap485","_id":"ciypwm3cg005y21sv6xgpjf9v"},{"post_id":"ciypwm3bg004n21sv7l1ddxrz","category_id":"ciypwm3c0005621svwg9s5r8n","_id":"ciypwm3ch006221sv5e2oylst"},{"post_id":"ciypwm3bj004r21svidlm72tf","category_id":"ciypwm3c6005e21sv23z2ug6y","_id":"ciypwm3ch006421svxmtw5qim"},{"post_id":"ciypwm3bz005521sv6q6whiyk","category_id":"ciypwm3cd005m21sva7pdlkq9","_id":"ciypwm3ci006721svt5y7rxqp"},{"post_id":"ciypwm3c2005921sv50p1s021","category_id":"ciypwm3ce005u21sv403nox6t","_id":"ciypwm3ci006a21svs32vbnui"},{"post_id":"ciypwm3c4005d21sv93q4gepl","category_id":"ciypwm3cg006021svbgr4bjye","_id":"ciypwm3cj006d21svikgkoqz0"},{"post_id":"ciypwm3c7005h21svamd3gs1p","category_id":"ciypwm3cg006021svbgr4bjye","_id":"ciypwm3cj006g21sv20sxx8yw"},{"post_id":"ciypwm3cb005l21svlf0gc6a4","category_id":"ciypwm3ci006c21svajl2sgwk","_id":"ciypwm3cj006j21sv3m3o43yb"}],"PostTag":[{"post_id":"ciypwm370000621sv5wqww7o5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37a000a21svfax58516"},{"post_id":"ciypwm36l000121sv4vi8ieps","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37f000d21sv7fk525fo"},{"post_id":"ciypwm373000721sve8sra0p8","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37k000i21svnad3ijp2"},{"post_id":"ciypwm36p000221svq1phbbk0","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37n000l21svsaspgr17"},{"post_id":"ciypwm37g000e21sv4w08jqwz","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37r000q21svw41qxuqt"},{"post_id":"ciypwm36y000521svf4gipt4m","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37u000s21svdyy1ze7s"},{"post_id":"ciypwm37s000r21svd81wuly4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37y000x21svbru4huzo"},{"post_id":"ciypwm37v000u21svtfny6hag","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm380001021svtybgpqsz"},{"post_id":"ciypwm37z000y21sv7ku7mwqz","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm386001521svutmja09k"},{"post_id":"ciypwm37b000b21svs9suqb35","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38a001721svbmdelerh"},{"post_id":"ciypwm37b000b21svs9suqb35","tag_id":"ciypwm37y000w21sv7akqrkp4","_id":"ciypwm38f001c21svvt4h2mm7"},{"post_id":"ciypwm381001121svtap4yfqx","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38h001e21svn5ld5b06"},{"post_id":"ciypwm387001621sv0tyyvktn","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38k001i21svqkuzu8tv"},{"post_id":"ciypwm38b001821svqerj6t12","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38o001m21svg8cviaig"},{"post_id":"ciypwm38f001d21svebn6o2x0","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38s001p21svh9m4ut1m"},{"post_id":"ciypwm37k000j21svx26uzv21","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38x001t21svfn04cqko"},{"post_id":"ciypwm37k000j21svx26uzv21","tag_id":"ciypwm38f001b21svlvklr72p","_id":"ciypwm390001w21svzftkbwhz"},{"post_id":"ciypwm38t001s21svt0sqa4iq","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm394002021svo8n48ckh"},{"post_id":"ciypwm37n000m21svlowv6kfh","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm396002321sva7axz9ya"},{"post_id":"ciypwm37n000m21svlowv6kfh","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm39a002721sv1pflqbvt"},{"post_id":"ciypwm391001y21svngtxbmr2","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm39c002a21svaw84tgku"},{"post_id":"ciypwm394002121svo7zb0zb3","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39e002e21svowo1d3vo"},{"post_id":"ciypwm38h001f21sv8w2usj1k","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39i002h21svuf36v2oh"},{"post_id":"ciypwm396002521svmkmo7uob","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39l002m21svnll9m813"},{"post_id":"ciypwm39a002821svvdj89we5","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39o002p21svotoxav00"},{"post_id":"ciypwm38l001k21svmq9vzpvt","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39s002t21sv69d0gzxb"},{"post_id":"ciypwm39e002f21svqd89nzvh","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39x002w21svj85eaeh5"},{"post_id":"ciypwm38y001u21svs7mji43n","tag_id":"ciypwm39e002d21sv64407bht","_id":"ciypwm3a2003121svv17iivnk"},{"post_id":"ciypwm39i002i21sv9msto5pk","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ae003421svlmlryek5"},{"post_id":"ciypwm39m002n21svzvub4bz1","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3ah003821svmtv2qne3"},{"post_id":"ciypwm39c002b21svtsnp2crr","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ak003c21svikhitx11"},{"post_id":"ciypwm39c002b21svtsnp2crr","tag_id":"ciypwm39l002k21svytq32fhf","_id":"ciypwm3am003g21sv3pyhg7yw"},{"post_id":"ciypwm39p002q21svs69ki7jr","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ao003k21sv2n3jtmop"},{"post_id":"ciypwm3ae003521svyblhs7lq","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3ar003o21sve5zw7sjz"},{"post_id":"ciypwm39s002u21svylq4icy5","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3av003r21svguznlq80"},{"post_id":"ciypwm39s002u21svylq4icy5","tag_id":"ciypwm3a1002z21svtd90b889","_id":"ciypwm3ax003v21svqkqk8ot1"},{"post_id":"ciypwm3ai003921sv5f0f4edv","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3b1003z21svx4pe0b8t"},{"post_id":"ciypwm3al003d21svs6i4yavl","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3b4004321sv6e3a8xso"},{"post_id":"ciypwm39x002x21svqqrk3m16","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3b6004621sv1joiocxx"},{"post_id":"ciypwm39x002x21svqqrk3m16","tag_id":"ciypwm3ak003a21svg0ygjid2","_id":"ciypwm3b9004b21svgaibm1bp"},{"post_id":"ciypwm3an003h21sv7b0gw30l","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3bc004e21svh9h6tpw1"},{"post_id":"ciypwm3an003h21sv7b0gw30l","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm3be004i21svu5c090n6"},{"post_id":"ciypwm3ao003l21svrwyk2i8l","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3bg004m21sv3asght9w"},{"post_id":"ciypwm3a2003221svf7toen7b","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3bi004p21svf6eui4kf"},{"post_id":"ciypwm3a2003221svf7toen7b","tag_id":"ciypwm3ao003i21svh8r94vgn","_id":"ciypwm3bo004t21sveo6cxfit"},{"post_id":"ciypwm3ay003w21svpn73kvxk","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3br004w21svoy8j8l2c"},{"post_id":"ciypwm3ay003w21svpn73kvxk","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm3bv005121svtvhgykh1"},{"post_id":"ciypwm3b4004421svqhoqjae5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3by005421svwqniij1v"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3c1005821svzddb5nmn"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","tag_id":"ciypwm3b1003x21svxhs5szvn","_id":"ciypwm3c4005c21sv1swcumb5"},{"post_id":"ciypwm3ba004c21svocwumvw5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3c7005g21sv5397cxqt"},{"post_id":"ciypwm3b2004021svioptmeei","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cb005k21svp39qfahn"},{"post_id":"ciypwm3b2004021svioptmeei","tag_id":"ciypwm3b8004821sva121xkab","_id":"ciypwm3cd005o21svt9yhmohq"},{"post_id":"ciypwm3bc004f21svgewuytn4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cd005r21sv2id72j8f"},{"post_id":"ciypwm3be004k21svfutfhg13","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ce005t21sv1bcnptke"},{"post_id":"ciypwm3b7004721sv1225ic1c","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ce005x21svaaixto5z"},{"post_id":"ciypwm3b7004721sv1225ic1c","tag_id":"ciypwm3b8004821sva121xkab","_id":"ciypwm3cg005z21sv5izyd5fu"},{"post_id":"ciypwm3bg004n21sv7l1ddxrz","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ch006321svis6jaywg"},{"post_id":"ciypwm3bj004r21svidlm72tf","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ch006521svaju372ai"},{"post_id":"ciypwm3bw005221sv9xsoxqtn","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ci006821sv2jnf3mrg"},{"post_id":"ciypwm3bz005521sv6q6whiyk","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ci006b21svxcw5s4lc"},{"post_id":"ciypwm3bo004u21sve7kv8amn","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3cj006e21sv7st5k6qa"},{"post_id":"ciypwm3bo004u21sve7kv8amn","tag_id":"ciypwm3bv004z21sveko5655v","_id":"ciypwm3cj006f21svanymt0fg"},{"post_id":"ciypwm3c7005h21svamd3gs1p","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cj006h21sv8g5sjcth"},{"post_id":"ciypwm3cb005l21svlf0gc6a4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cj006i21sv1tsxgij9"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3c3005a21sv2ovu098c","_id":"ciypwm3cj006k21sv64jn4hkr"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3ca005i21svhxwhovao","_id":"ciypwm3cj006l21svqskl39j0"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3bv004z21sveko5655v","_id":"ciypwm3cj006m21sv7vrhnwhx"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3ce005v21svhpc2mlny","_id":"ciypwm3cj006n21sv6iyou2rp"},{"post_id":"ciypwm3c2005921sv50p1s021","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3cj006o21svh2tjl2ht"},{"post_id":"ciypwm3c2005921sv50p1s021","tag_id":"ciypwm3ch006121svvkfcwhkd","_id":"ciypwm3cj006p21sv7bmlqcn8"},{"post_id":"ciypwm3c4005d21sv93q4gepl","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ck006q21svda2o4obb"},{"post_id":"ciypwm3c4005d21sv93q4gepl","tag_id":"ciypwm3ci006921sv33rfphoh","_id":"ciypwm3ck006r21svm3332mu0"}],"Tag":[{"name":"portal","_id":"ciypwm36x000421svx7frtbsl"},{"name":"core","_id":"ciypwm37r000o21svjcqqxc6i"},{"name":"cloudfoundry","_id":"ciypwm37y000w21sv7akqrkp4"},{"name":"openshift","_id":"ciypwm38f001b21svlvklr72p"},{"name":"openstack","_id":"ciypwm38s001r21svydvoae1d"},{"name":"detail","_id":"ciypwm392001z21svrcw0otfu"},{"name":"cookbook","_id":"ciypwm39e002d21sv64407bht"},{"name":"filesystem","_id":"ciypwm39l002k21svytq32fhf"},{"name":"security","_id":"ciypwm3a1002z21svtd90b889"},{"name":"network","_id":"ciypwm3ak003a21svg0ygjid2"},{"name":"storage","_id":"ciypwm3ao003i21svh8r94vgn"},{"name":"swarm","_id":"ciypwm3b1003x21svxhs5szvn"},{"name":"architect","_id":"ciypwm3b8004821sva121xkab"},{"name":"django","_id":"ciypwm3bv004z21sveko5655v"},{"name":"install","_id":"ciypwm3c3005a21sv2ovu098c"},{"name":"python","_id":"ciypwm3ca005i21svhxwhovao"},{"name":"docker","_id":"ciypwm3ce005v21svhpc2mlny"},{"name":"rails","_id":"ciypwm3ch006121svvkfcwhkd"},{"name":"http","_id":"ciypwm3ci006921sv33rfphoh"}]}}